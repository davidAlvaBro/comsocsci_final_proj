{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import copy\n",
    "#import community\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess the two book dataframes\n",
    "- Load dataframes ('https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home', 'https://github.com/malcolmosh/goodbooks-10k-extended/blob/master/README.md')\n",
    "- Remove faulty elements of dataframe without the descriptions\n",
    "- Remove \"additions\" to titles in the description dataframes - Example: title (series #1) -> title\n",
    "- Make a new dataframe that contains \"Book ID\" (from the dataframe without descriptions), \"Book Title\" (-||-), \"genre\" (description dataframe), \"description\" (description dataframe)\n",
    "- Store this dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "descriptions_df = pd.read_csv(DATA_PATH + \"books_descriptions.csv\")\n",
    "book_ID_df = pd.read_json(DATA_PATH + 'goodreads_book_works.json', lines=True)\n",
    "\n",
    "# Remove columns that are not needed in book_ID_df\n",
    "book_ID_df = book_ID_df.filter(items=[\"best_book_id\", \"original_title\", \"reviews_count\"])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List amount of books are in each dataframe\n",
    "print(f\"Amount of books in descriptions_df: {len(descriptions_df)}\")\n",
    "print(f\"Amount of books in book_ID_df: {len(book_ID_df)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove faulty elements from the book_ID_df\n",
    "book_ID_df = book_ID_df[book_ID_df['original_title'] != '']\n",
    "\n",
    "print(f\"Amount of books in book_ID_df: {len(book_ID_df)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove faulty elements from the description dataframe\n",
    "descriptions_df = descriptions_df[descriptions_df['description'].apply(lambda x: isinstance(x, str))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove \"additions\" to titles in the descriptions_df\n",
    "for i, row in tqdm(descriptions_df.iterrows()):\n",
    "    original_title = row[\"title\"]\n",
    "    new_title = re.sub(r'\\((.*)', '', original_title)\n",
    "    descriptions_df.at[i, \"title\"] = new_title.strip()\n",
    "\n",
    "# Check if it worked -> it did\n",
    "# descriptions_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lower case all titles to not have confusion in this manner\n",
    "descriptions_df['title'] = descriptions_df['title'].str.lower()\n",
    "book_ID_df['original_title'] = book_ID_df['original_title'].str.lower()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find corresponding indexes to merge the dataframes\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "titles_not_found = []\n",
    "book_df = pd.DataFrame(columns=['book_id', 'title', 'description', 'genres'])\n",
    "\n",
    "for i, row in tqdm(descriptions_df.iterrows(), total = descriptions_df.shape[0]):\n",
    "    title = row['title']\n",
    "    # Check if the title is in book_ID_df, else append it to the titles_not_found list\n",
    "    if title in book_ID_df['original_title'].values:\n",
    "        # Get all rows that have the a matching title as the current row\n",
    "        temp_df = book_ID_df[book_ID_df['original_title'] == title]\n",
    "\n",
    "        # Get the book id of the book  with the highest amount of reviews\n",
    "        book_id = temp_df['best_book_id'][temp_df['reviews_count'].idxmax()]\n",
    "        descriptions = row['description']\n",
    "        genres = row['genres']\n",
    "        book_df = book_df.append({'book_id': book_id, 'title': title, 'description': descriptions, 'genres': genres}, ignore_index=True)\n",
    "    else:\n",
    "        titles_not_found.append(title)\n",
    "\n",
    "# print the amount of elements that are not found\n",
    "print(f\"Out of the 10000 titles {len(titles_not_found)} are not found in the book_ID_df\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Somehow the same book appears multiple times, hence we drop the duplicates\n",
    "book_df.drop_duplicates(subset=['title'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# TODO - Possibly add the capacity to look through titles and determine if a shorter version could be found in the other dataset (\"harry potter and the sorcersers stone\" becoming \"harry potter and the \" and possibly finding harry potter and the philosophers stone\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess the shelves\n",
    "- Use \"book_id_map.csv\" to find the books we use (ids) and store \"new_ids\" (the ids we can use to find the relevant shelfs)\n",
    "- Drop all rows in \"goodreads_interactions.csv\" that have different ids than \"new_ids\".\n",
    "- Store this dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "book_id_map_df = pd.read_csv(DATA_PATH + \"book_id_map.csv\")\n",
    "book_df = pd.read_csv(DATA_PATH + \"book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create map from book_id to book_id_csv\n",
    "book_id_map = {book_id_map_df['book_id'][i]: book_id_map_df['book_id_csv'][i] for i in range(len(book_id_map_df))}\n",
    "\n",
    "# Change the book ID in our dataset to match the shelf dataset\n",
    "remove_list = [] # remove about 15 books that are for inexplicable reasons not in the shelf dataset\n",
    "for i in range(len(book_df)):\n",
    "    try:\n",
    "        book_df[\"book_id\"][i] = book_id_map[book_df[\"book_id\"][i]]\n",
    "    except:\n",
    "        remove_list.append(i)\n",
    "print(f\"Out of {i} books {len(remove_list)} are not in the shelf dataset and hence removed\")\n",
    "book_df.drop(remove_list, inplace=True)\n",
    "\n",
    "# Save the book_df dataframe with the index change\n",
    "book_df.to_csv(DATA_PATH + 'book_matching_ids_df.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the shelves dataframe (this takes some time and memory)\n",
    "shelves_df = pd.read_csv(DATA_PATH + \"goodreads_interactions.csv\")\n",
    "\n",
    "# Check how many books are on the shelves of ALL users combined\n",
    "print(f'There are {len(shelves_df)} books, copies counted aswell')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove books on the shelves that are not in the book_df\n",
    "shelves_df = shelves_df[shelves_df['book_id'].isin(book_df['book_id'].tolist())]\n",
    "\n",
    "# Save the new shelves_df\n",
    "shelves_df.to_csv(DATA_PATH + 'shelves_df.csv', index=False)\n",
    "\n",
    "# Check how many books are on the shelves of ALL users combined - after removal of books not in book_df\n",
    "print(f\"We have {len(shelves_df)} shelves in total, and in these there are {len(set(shelves_df['book_id'].tolist()))} unique books.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TF-IDF embeddings\n",
    "- Create TF-IDF embeddings\n",
    "- Create them for genres aswell"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports for the text analysis\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Import the book description dataframe\n",
    "book_df = pd.read_csv(DATA_PATH + \"book_matching_ids_df.csv\") # TODO help me not make a \"Unnamed: 0\" column... I want to use the book_id as index, but then it creates this column\n",
    "\n",
    "# Logorithmic scale chosen for IDF\n",
    "BASE = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Function to clean strings (from week 7)\n",
    "def clean_strings(strings):\n",
    "    \"\"\" Cleans a list of strings by removing URLs, numbers, punctuation and stop words\n",
    "\n",
    "    Args:\n",
    "    - strings: a list of strings\n",
    "\n",
    "    returns:\n",
    "    - cleaned_strings: a list of cleaned strings\n",
    "    \"\"\"\n",
    "    cleaned_strings = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for string in strings:\n",
    "        # Remove URLs\n",
    "        string = re.sub(r'http\\S+', '', string)\n",
    "\n",
    "        # Remove numbers\n",
    "        string = re.sub(r'[0-9]', '', string)\n",
    "\n",
    "        # Keep only what is not punctuation\n",
    "        string = re.sub(r'[^\\w\\s]', '', string)\n",
    "\n",
    "        # Lowercase\n",
    "        string = string.lower()\n",
    "\n",
    "        # Remove empty strings and remove stop words and moke them\n",
    "        if len(string) and string not in stop_words and type(string) == str:\n",
    "            cleaned_strings.append(string)\n",
    "\n",
    "    return cleaned_strings\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create tokens from descriptions\n",
    "# If the dataframe has not yet gotten the tokens, do it here and save it\n",
    "if not 'tokens' in book_df.columns:\n",
    "    book_df['tokens'] = None\n",
    "    for i, row in tqdm(book_df.iterrows(), total = book_df.shape[0]):\n",
    "        description = row['description']\n",
    "        # If the description is not a string, it is probably a NaN, so we set it to None\n",
    "        if type(description) == str:\n",
    "            tokens = nltk.word_tokenize(description)\n",
    "            clean_tokens = clean_strings(tokens)\n",
    "            book_df['tokens'][i] = clean_tokens # str(clean_tokens)\n",
    "\n",
    "    book_df.to_csv(DATA_PATH + 'book_matching_ids_df.csv', index=False)\n",
    "else:\n",
    "    book_df['tokens'] = book_df['tokens'].apply(ast.literal_eval)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the tf scores for each community (from the previous weeks)\n",
    "def TF_from_corpus(corpus):\n",
    "    \"\"\" Calculates the TF scores for each word in the corpus\n",
    "\n",
    "    Args:\n",
    "        corpus (list): list of lists of words/strings\n",
    "\n",
    "    Returns:\n",
    "        TF_df (pandas.DataFrame): Dataframe containing the TF scores for each word in the corpus\n",
    "    \"\"\"\n",
    "    # Create empty dictionary to keep track of word counts\n",
    "    word_counts = {}\n",
    "    n_communities = len(corpus)\n",
    "\n",
    "    # Iterate through all communities\n",
    "    for i, document in tqdm(enumerate(corpus), total=n_communities):\n",
    "        # Iterate through each word in the current sublist\n",
    "        for word in document:\n",
    "            # If the current word is not in the dictionary, add it with a list of zeros\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = [0] * n_communities\n",
    "\n",
    "            # Increment count for the current word and list index\n",
    "            word_counts[word][i] += 1\n",
    "\n",
    "    # Create pandas dataframe from the word_counts dictionary\n",
    "    TF_df = pd.DataFrame.from_dict(word_counts).transpose()\n",
    "\n",
    "    return TF_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function that takes in a TF and an IDF and computes the TF_IDF dataframe (from the previous weeks)\n",
    "def make_TF_IDF(TF_df, IDF_dict):\n",
    "    \"\"\"Multiply the TF and IDF scores to get the TF-IDF scores\n",
    "\n",
    "    Args:\n",
    "        TF_df (pandas.DataFrame): Dataframe containing the TF scores for each word in the corpus\n",
    "        IDF_dict (dict): Dictionary containing the IDF scores for each word in the corpus\n",
    "\n",
    "    Returns:\n",
    "        TF_IDF (pandas.DataFrame): Dataframe containing the TF-IDF scores for each word in the corpus\n",
    "    \"\"\"\n",
    "    # Create the TF-IDF dataframe\n",
    "    TF_IDF = pd.DataFrame(index=TF_df.index, columns=TF_df.columns)\n",
    "\n",
    "    # iterate over the index of the DataFrame\n",
    "    for word in tqdm(TF_df.index, total=TF_df.shape[0]):\n",
    "        # multiply the values by the IDF_dict value\n",
    "        TF_IDF.loc[word] = TF_df.loc[word] * IDF_dict[word]\n",
    "\n",
    "\n",
    "    return TF_IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the TF dataframe for the corpus if possible, else create it\n",
    "try:\n",
    "    TF_book_df = pd.read_csv(DATA_PATH + \"TF_book_df.csv\", index_col=0)\n",
    "except:\n",
    "    # Create it\n",
    "    TF_book_df = TF_from_corpus(book_df['tokens'])\n",
    "\n",
    "    # Rename the columns to the book_ids\n",
    "    TF_book_df.columns = book_df['book_id'].tolist()\n",
    "\n",
    "    # Save the TF dataframe\n",
    "    TF_book_df.to_csv(DATA_PATH + \"TF_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the total token count \"T_all_books\" and the IDF score for each book \"IDF_book_dict\"\n",
    "try:\n",
    "    IDF_dict = np.load(DATA_PATH + 'IDF_dict.npy', allow_pickle=True).item()\n",
    "except:\n",
    "    T_all_books = TF_book_df.apply(lambda row: (row != 0).sum(), axis=1)\n",
    "    # The log BASE is chosen when loading the libraries\n",
    "    IDF_dict = {word: np.emath.logn(BASE, len(TF_book_df.columns)/ T_all_books[word]) for word in TF_book_df.index}\n",
    "\n",
    "    np.save(DATA_PATH + 'IDF_dict.npy', IDF_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the TF-IDF scores for each book \"TF_IDF_book_df\" if it has not already been made\n",
    "try:\n",
    "    TF_IDF_book_df = pd.read_csv(DATA_PATH + \"TF_IDF_book_df.csv\", index_col=0)\n",
    "except:\n",
    "    # Create the dataframe\n",
    "    TF_IDF_book_df = make_TF_IDF(TF_book_df, IDF_dict)\n",
    "\n",
    "    # Save the dataframe\n",
    "    TF_IDF_book_df.to_csv(DATA_PATH + \"TF_IDF_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Genre TF and TF_IDF scores\n",
    "- Do this by having all books with a genre define the \"document\" for that genre\n",
    "- Then compute the \"TF_genre_df\" dataframe, by summing all books from \"TF_book_df\" from that genre\n",
    "- Here we make the decision that the IDF is the same as for the books.\n",
    "    - (Alternatively one could have weighed each book and made a new IDF score, however, this weighs a book with twice as many genres twice as large, hence we use the other option)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the set of genres\n",
    "genres = set()\n",
    "for i in book_df[\"genres\"].to_list():\n",
    "    genres = genres.union(set(ast.literal_eval(i)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try to load the TF_genres_df, else create it\n",
    "try:\n",
    "    # Load the TF_genres_df\n",
    "    TF_genres_df = pd.read_csv(DATA_PATH + \"TF_genres_df.csv\", index_col=0)\n",
    "except:\n",
    "    # For each genre, sum all TF scores for books in that genre\n",
    "    TF_genres_df = pd.DataFrame(index=TF_book_df.index, columns=genres)\n",
    "\n",
    "    TF_genres_df = TF_genres_df.fillna(0)\n",
    "\n",
    "    # Go through all books and add the TF scores to the genres of the book\n",
    "    for i, row in tqdm(book_df.iterrows()):\n",
    "        for genre in ast.literal_eval(row['genres']):\n",
    "            TF_genres_df[genre] = TF_genres_df[genre] + TF_book_df[row['book_id']]\n",
    "\n",
    "    # Save the genres_TF_df\n",
    "    TF_genres_df.to_csv(DATA_PATH + \"TF_genres_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try to load the TF_IDF_genres_df, else create it\n",
    "try:\n",
    "    # Load the TF_IDF_genres_df\n",
    "    TF_IDF_genres_df = pd.read_csv(DATA_PATH + \"TF_IDF_genres_df.csv\", index_col=0)\n",
    "except:\n",
    "    # Create the dataframe\n",
    "    TF_IDF_genres_df = make_TF_IDF(TF_genres_df, IDF_dict)\n",
    "\n",
    "    # Save the dataframe\n",
    "    TF_IDF_genres_df.to_csv(DATA_PATH + \"TF_IDF_genres_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Have not added things with # delete"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a genre for each book\n",
    "- make the inner product which each book and the genre vector (both normed)\n",
    "- let the largest inner product that the book contains be the genre of the book"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Inner product function\n",
    "def inner_product(v1, v2):\n",
    "    \"\"\"Calculates the normed inner product of two vectors\n",
    "\n",
    "    Args:\n",
    "        v1 (list): list of numbers\n",
    "        v2 (list): list of numbers\n",
    "\n",
    "    Returns:\n",
    "        inner_product (float): inner product of the two vectors (divided by the product of their norms)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Generate genre by taking inner products between each book and each genre of that book, and choosing the maximal\n",
    "def get_genres(book_df, TF_IDF_genres_df, TF_IDF_book_df):\n",
    "    # TODO: Check these args and return\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        book_df (DataFrame): DataFrame with genres for each book\n",
    "        TF_IDF_genres_df (DataFrame): DataFrame with TF_IDF each genre\n",
    "        TF_IDF_book_df (DataFrame): DataFrame with TF_IDF each book\n",
    "\n",
    "    Returns:\n",
    "        genres (dict): Dictionary with book ids as keys and the corresponding gerne that matches the best\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary to store the genre for each book\n",
    "    genres = {}\n",
    "\n",
    "    # Go through each book\n",
    "    for i, row in tqdm(book_df.iterrows()):\n",
    "        # Get the TF-IDF scores for the current book\n",
    "        book_TF_IDF = TF_IDF_book_df[row[\"book_id\"]]\n",
    "\n",
    "        best_genre = (None, 0)\n",
    "\n",
    "        # Go through each genre of the book\n",
    "        for genre in ast.literal_eval(row['genres']):\n",
    "            genre_TF_IDF = TF_IDF_genres_df[genre]\n",
    "            # print(f\"inner_product {inner_product(book_TF_IDF, genre_TF_IDF)} {genre}\") # Testing\n",
    "            if inner_product(book_TF_IDF, genre_TF_IDF) > best_genre[1]:\n",
    "                best_genre = (genre, inner_product(book_TF_IDF, genre_TF_IDF))\n",
    "        # print(best_genre) # Testing\n",
    "        # break # Testing\n",
    "        # Save the best genre\n",
    "        genres[row[\"book_id\"]] = best_genre[0]\n",
    "\n",
    "    return genres"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the genres for each book\n",
    "genres = get_genres(book_df, TF_IDF_genres_df, TF_IDF_book_df)\n",
    "\n",
    "book_df[\"top_genre\"] = book_df[\"book_id\"].map(genres)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the book_df as complete_book_df\n",
    "book_df.to_csv(DATA_PATH + \"complete_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create network\n",
    "- Load dataframe with shelves\n",
    "- Implode dataframe to make shelves into edgelist\n",
    "- Calculate assortativity\n",
    "    - Genre\n",
    "    - Degrees\n",
    "- Get largest sub network\n",
    "- Create communities\n",
    "    - Color according to genre"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Create the network\n",
    "def create_network(threshhold=0.2):\n",
    "    \"\"\"\n",
    "    Creates the network from the shelf edges.\n",
    "    If the edges have already been created, they are loaded.\n",
    "    Otherwise, they are created and stored. The edges are filtered by the threshhold. The network is returned.\n",
    "\n",
    "    Args:\n",
    "        threshhold (int): The minimum number of times an edge must appear to be included in the network.\n",
    "\n",
    "    Returns:\n",
    "        G (networkx.Graph): The network of book edges.\n",
    "    \"\"\"\n",
    "    # If the edges have already been created, load them\n",
    "    try:\n",
    "        # Load the shelf edges\n",
    "        edges = np.load(DATA_PATH + 'shelf_edges.npy', allow_pickle=True).item() # (takes about 10 minutes)\n",
    "    except:\n",
    "        # Create the shelf edges and store them (this takes about 12 hours)\n",
    "        shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')\n",
    "        # Collapse into shelves\n",
    "        imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "\n",
    "        # Count occurrences of each book\n",
    "        book_counts = {}\n",
    "        for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "            for book in shelf['book_id']:\n",
    "                if book in book_counts:\n",
    "                    book_counts[book] += 1\n",
    "                else:\n",
    "                    book_counts[book] = 1\n",
    "\n",
    "        # Count appearances of pairs of books\n",
    "        edges = {}\n",
    "        for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "            if len(shelf['book_id']) > 1:\n",
    "                # Generate pairs of books\n",
    "                for i in range(len(shelf['book_id'])):\n",
    "                    for j in range(i+1, len(shelf['book_id'])):\n",
    "                        # Create edge\n",
    "                        edge = frozenset([shelf['book_id'][i], shelf['book_id'][j]])\n",
    "                        # Add edge to dictionary\n",
    "                        if edge in edges:\n",
    "                            edges[edge] += 1\n",
    "                        else:\n",
    "                            edges[edge] = 1\n",
    "\n",
    "        # Weigh edges by the inverse of the root of the number of times each book appears\n",
    "        for edge in edges:\n",
    "            edges[edge] = edges[edge] / (book_counts[list(edge)[0]] * book_counts[list(edge)[1]])**0.5\n",
    "\n",
    "        # Store the edges\n",
    "        np.save(DATA_PATH + 'shelf_edges.npy', edges)\n",
    "\n",
    "    # Remove edges that are below the threshhold\n",
    "    edges = {k: v for k, v in edges.items() if v >= threshhold}\n",
    "\n",
    "    # Create the network\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges.keys())\n",
    "\n",
    "    return G\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: omskrevet til at læse/skrive netværk i steder for pickle\n",
    "# Load/Generate the network\n",
    "try:\n",
    "    # Load the network\n",
    "    G = nx.read_graphml(DATA_PATH + \"shelves_network_04.graphml\")\n",
    "except:\n",
    "    # Make the network\n",
    "    G = create_network() #TODO: stod før 2000 her i ()\n",
    "\n",
    "    # Save the network\n",
    "    nx.write_graphml(G, DATA_PATH + \"shelves_network.graphml\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load data\n",
    "if shelves_df not in locals():\n",
    "    shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Implode dataframe to get books for each user in a list for each\n",
    "imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "print(f'The amount of users are: {len(imploded_df)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary edges and appearances\n",
    "edges = {}\n",
    "\n",
    "# Go through shelves to get edges\n",
    "for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "    if len(shelf['book_id']) > 1:\n",
    "        # Generate pairs of books\n",
    "        for i in range(len(shelf['book_id'])):\n",
    "            for j in range(i+1, len(shelf['book_id'])):\n",
    "                # Create edge\n",
    "                edge = frozenset([shelf['book_id'][i], shelf['book_id'][j]])\n",
    "                # Add edge to dictionary\n",
    "                if edge in edges:\n",
    "                    edges[edge] += 1\n",
    "                else:\n",
    "                    edges[edge] = 1\n",
    "\n",
    "print(f'The amount of edges are {len(edges)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Comment on this being to much in rapport or that is was to much (before threshold)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the edges\n",
    "np.save(DATA_PATH + 'shelf_edges.npy', edges)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: Jeg tror ikke at det her er ordenligt pushet eftersom det er en pickle og ikke en npy, og har derfor heller ikke tilføjet resten af \"Generate network\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the shelf edges\n",
    "edges = np.load(DATA_PATH + 'shelf_edges.npy', allow_pickle=True).item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot a histogram of the number of times each edge appears\n",
    "plt.hist(edges.values(), log=True, bins=100)\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('People who had booth books')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of books appearing together')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#TODO: noget threshold jeg ikke forstår da det er med i funktionen \"create_network\" skal bare have hjælp herfra"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count occurrences of each book\n",
    "book_counts = {}\n",
    "for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "    for book in shelf['book_id']:\n",
    "        if book in book_counts:\n",
    "            book_counts[book] += 1\n",
    "        else:\n",
    "            book_counts[book] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot a histogram of the number of times each book appears\n",
    "plt.hist(book_counts.values(), log=True, bins=100)\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('People who had the book')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of books appearing together')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: Show how to make communities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import netwulf\n",
    "import ast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Used to make all node ids ints\n",
    "def make_all_nodes_ints(graph):\n",
    "    #TODO: args and returns\n",
    "\n",
    "    # Check if first id is a string, in which case, don't do anyting\n",
    "    if isinstance(list(graph.nodes())[0], int):\n",
    "            return graph\n",
    "\n",
    "    # Make a mapping so each node can be renamed into what it was, but a string instead\n",
    "    node_map = {}\n",
    "    for node in graph.nodes():\n",
    "        node_map[node] = int(node)\n",
    "    str_graph = nx.relabel_nodes(graph, node_map)\n",
    "\n",
    "    return str_graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "shelf_graph = nx.read_graphml(DATA_PATH + 'shelves_graph_04.graphml')\n",
    "shelf_graph = make_all_nodes_ints(shelf_graph)\n",
    "\n",
    "NLP_graph = nx.read_graphml(DATA_PATH + 'NLP_graph_04.graphml')\n",
    "NLP_graph = make_all_nodes_ints(NLP_graph)\n",
    "\n",
    "#Load communities\n",
    "shelf_louvain = np.load(DATA_PATH + 'shelves_communities_04.npy', allow_pickle = True)\n",
    "NLP_louvain = np.load(DATA_PATH + 'NLP_communities_04.npy', allow_pickle= True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if not \"complete_book_df\" in locals():\n",
    "    complete_book_df = pd.read_csv(DATA_PATH + \"complete_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def make_attributes(df, graph):\n",
    "    #TODO: args and returns\n",
    "    # make a dictionary with attributes for each node\n",
    "    book_attributes = dict()\n",
    "    for i, book in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        node = str(book['book_id'])\n",
    "        if not node in graph.nodes():\n",
    "            continue\n",
    "        book_attributes[node] = dict()\n",
    "        top_genre = book['top_genre']\n",
    "        title = book['title']\n",
    "        genres = ast.literal_eval(book['genres'])\n",
    "\n",
    "        book_attributes[node]['title'] = title\n",
    "        book_attributes[node]['genres'] = genres\n",
    "        book_attributes[node]['top_genre'] = top_genre\n",
    "    return book_attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7676/7676 [00:00<00:00, 21281.12it/s]\n",
      "100%|██████████| 7676/7676 [00:00<00:00, 21288.14it/s]\n"
     ]
    }
   ],
   "source": [
    "shelf_attribute_dict = make_attributes(complete_book_df, shelf_graph)\n",
    "NLP_attribute_dict = make_attributes(complete_book_df, NLP_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "nx.set_node_attributes(shelf_graph, shelf_attribute_dict)\n",
    "nx.set_node_attributes(NLP_graph, NLP_attribute_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "# List of colors to use for the visualization\n",
    "\n",
    "\n",
    "# Use netwulf to visualize the communities\n",
    "def visualize_communities(graph, communities):\n",
    "    colors = list(mcolors.CSS4_COLORS.values())[20:len(communities) + 20]\n",
    "    # Create a dictionary that maps nodes to communities\n",
    "    node_to_community = {}\n",
    "    for i, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            node = str(node)\n",
    "            node_to_community[node] = i\n",
    "\n",
    "    # Add a color attribute to the nodes\n",
    "    for node in graph.nodes:\n",
    "        # Add the community number as a node attribute\n",
    "        graph.nodes[node][\"color\"] = colors[node_to_community[node]]\n",
    "\n",
    "    # Visualize the graph with netwulf\n",
    "    netwulf.visualize(graph, config={\"Node color\": \"color\",\n",
    "                                    'node_size': 50,\n",
    "                                    'link_width': 0.1,\n",
    "                                    'link_alpha': 0.01,\n",
    "                                    'node_size_variation': 0.3,\n",
    "                                    'zoom': 0.65,\n",
    "                                    'scale_node_size_by_strength': True,})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "visualize_communities(shelf_graph, shelf_louvain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "visualize_communities(NLP_graph, NLP_louvain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Colors for genre"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def create_genre_dict(graph):\n",
    "    genre_dict = {}\n",
    "    for node in tqdm(graph.nodes):\n",
    "        genre = nx.get_node_attributes(graph, 'top_genre')[node]\n",
    "        try:\n",
    "            genre_dict[genre].append(node)\n",
    "        except:\n",
    "            genre_dict[genre] = [node]\n",
    "    return genre_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7665/7665 [00:12<00:00, 612.85it/s]\n",
      "100%|██████████| 7671/7671 [00:12<00:00, 611.70it/s]\n"
     ]
    }
   ],
   "source": [
    "shelf_genre_dict = create_genre_dict(shelf_graph)\n",
    "NLP_genre_dict = create_genre_dict(NLP_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Use netwulf to visualize the genres\n",
    "def visualize_genres(graph, genre_dict):\n",
    "    colors = list(mcolors.CSS4_COLORS.values())[20:len(genre_dict.keys())+20]\n",
    "    # Create a dictionary that maps nodes to genre\n",
    "    node_to_genre = {}\n",
    "    for i, genre in enumerate(genre_dict.keys()):\n",
    "        for node in genre_dict[genre]:\n",
    "            node_to_genre[node] = i\n",
    "\n",
    "\n",
    "\n",
    "    # Add a color attribute to the nodes\n",
    "    for node in graph.nodes:\n",
    "        # Add the community number as a node attribute\n",
    "        graph.nodes[node][\"color\"] = colors[node_to_genre[node]]\n",
    "\n",
    "    # Visualize the graph with netwulf\n",
    "    netwulf.visualize(graph, config={\"Node color\": \"color\",\n",
    "                                    'node_size': 50,\n",
    "                                    'link_width': 0.1,\n",
    "                                    'link_alpha': 0.01,\n",
    "                                    'node_size_variation': 0.3,\n",
    "                                    'zoom': 0.65,\n",
    "                                    'scale_node_size_by_strength': True,})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "visualize_genres(shelf_graph, shelf_genre_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "visualize_genres(NLP_graph, NLP_genre_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate wordclouds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Find top three books for each community according to degree\n",
    "def get_top_3_books(graph_type, louvain_groups):\n",
    "    top_3_books = {}\n",
    "    # Store the top 3 books by degree for all communities\n",
    "    for i, community in enumerate(louvain_groups):\n",
    "\n",
    "        # Get the top 3 books by degree\n",
    "        sorted_dict = sorted(dict(graph_type.degree(community)).items(), key=lambda x: x[1], reverse=True)\n",
    "        names = nx.get_node_attributes(graph_type, \"title\")\n",
    "        top_3_keys = [k for k, v in sorted_dict[:3]] # Get the ID\n",
    "        top_3_names = [names[k] for k in top_3_keys] # get the name\n",
    "\n",
    "        top_3_books[i] = [(top_3_keys[j], top_3_names[j]) for j in range(len(top_3_keys))]\n",
    "        return top_3_books"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shelf graph\n",
      "**********\n",
      "1. largest community 1: \n",
      "Book: the hunger games, degree: 7312\n",
      "Book: to kill a mockingbird, degree: 6527\n",
      "Book: catching fire, degree: 5683\n",
      "\n",
      "2. largest community 0: \n",
      "Book: harry potter and the chamber of secrets, degree: 5694\n",
      "Book: harry potter and the prisoner of azkaban, degree: 5530\n",
      "Book: harry potter and the deathly hallows, degree: 5394\n",
      "\n",
      "3. largest community 5: \n",
      "Book: eclipse, degree: 2309\n",
      "Book: breaking dawn, degree: 2232\n",
      "Book: city of bones, degree: 1947\n",
      "\n",
      "4. largest community 3: \n",
      "Book: angels & demons, degree: 1024\n",
      "Book: a time to kill, degree: 151\n",
      "Book: along came a spider, degree: 145\n",
      "\n",
      "5. largest community 2: \n",
      "Book: the riddle, degree: 1384\n",
      "Book: a wrinkle in time, degree: 771\n",
      "Book: charlotte's web, degree: 657\n",
      "\n",
      "6. largest community 9: \n",
      "Book: fifty shades of grey, degree: 890\n",
      "Book: beautiful disaster, degree: 496\n",
      "Book: fifty shades darker, degree: 399\n",
      "\n",
      "7. largest community 6: \n",
      "Book: dark lover, degree: 498\n",
      "Book: dead until dark, degree: 405\n",
      "Book: halfway to the grave, degree: 291\n",
      "\n",
      "8. largest community 4: \n",
      "Book: the shining, degree: 322\n",
      "Book: the stand, degree: 259\n",
      "Book: dark visions, degree: 258\n",
      "\n",
      "9. largest community 7: \n",
      "Book: the tipping point: how little things can make a big difference, degree: 101\n",
      "Book: outliers: the story of success, degree: 96\n",
      "Book: freakonomics: a rogue economist explores the hidden side of everything, degree: 72\n",
      "\n",
      "NLP graph\n",
      "**********\n",
      "1. largest community 4: \n",
      "Book: abandon, degree: 667\n",
      "Book: fueled, degree: 535\n",
      "Book: wait for it, degree: 454\n",
      "\n",
      "2. largest community 8: \n",
      "Book: the greatest generation, degree: 527\n",
      "Book: private peaceful, degree: 364\n",
      "Book: the discoverers: a history of man's search to know his world and himself, degree: 318\n",
      "\n",
      "3. largest community 5: \n",
      "Book: the complete sherlock holmes, degree: 261\n",
      "Book: the fire within, degree: 220\n",
      "Book: the white princess, degree: 214\n",
      "\n",
      "4. largest community 6: \n",
      "Book: the guy not taken: stories, degree: 321\n",
      "Book: and then there were none, degree: 244\n",
      "Book: lipstick jungle, degree: 230\n",
      "\n",
      "5. largest community 3: \n",
      "Book: what we keep, degree: 370\n",
      "Book: suzanne's diary for nicholas, degree: 314\n",
      "Book: family pictures, degree: 306\n",
      "\n",
      "6. largest community 1: \n",
      "Book: the survivors club, degree: 271\n",
      "Book: i, alex cross, degree: 250\n",
      "Book: absolute fear, degree: 248\n",
      "\n",
      "7. largest community 0: \n",
      "Book: zoe's tale, degree: 242\n",
      "Book: one second after, degree: 202\n",
      "Book: shall we tell the president?, degree: 158\n",
      "\n",
      "8. largest community 7: \n",
      "Book: night world, no. 1, degree: 327\n",
      "Book: new moon, degree: 262\n",
      "Book: danse macabre, degree: 213\n",
      "\n",
      "9. largest community 2: \n",
      "Book: just one look, degree: 247\n",
      "Book: the sister, degree: 207\n",
      "Book: in too deep, degree: 200\n",
      "\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "shelf_int_graph = make_all_nodes_ints(shelf_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "shelf_top_3_books = get_top_3_books(shelf_int_graph, shelf_louvain)\n",
    "#print(shelf_top_3_books)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: [(968, 'harry potter and the chamber of secrets'),\n  (941, 'harry potter and the prisoner of azkaban'),\n  (613, 'harry potter and the deathly hallows')]}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shelf_top_3_books"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
