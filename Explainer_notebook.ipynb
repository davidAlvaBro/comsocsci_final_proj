{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import copy\n",
    "import pickle\n",
    "#import community\n",
    "import seaborn as sns\n",
    "import os\n",
    "import ast\n",
    "\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess the two book dataframes\n",
    "- Load dataframes ('https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home', 'https://github.com/malcolmosh/goodbooks-10k-extended/blob/master/README.md')\n",
    "- Remove faulty elements of dataframe without the descriptions\n",
    "- Remove \"additions\" to titles in the description dataframes - Example: title (series #1) -> title\n",
    "- Make a new dataframe that contains \"Book ID\" (from the dataframe without descriptions), \"Book Title\" (-||-), \"genre\" (description dataframe), \"description\" (description dataframe)\n",
    "- Store this dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "descriptions_df = pd.read_csv(DATA_PATH + \"books_descriptions.csv\")\n",
    "book_ID_df = pd.read_json(DATA_PATH + 'goodreads_book_works.json', lines=True)\n",
    "\n",
    "# Remove columns that are not needed in book_ID_df\n",
    "book_ID_df = book_ID_df.filter(items=[\"best_book_id\", \"original_title\", \"reviews_count\"])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List amount of books are in each dataframe\n",
    "print(f\"Amount of books in descriptions_df: {len(descriptions_df)}\")\n",
    "print(f\"Amount of books in book_ID_df: {len(book_ID_df)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove faulty elements from the book_ID_df\n",
    "book_ID_df = book_ID_df[book_ID_df['original_title'] != '']\n",
    "\n",
    "print(f\"Amount of books in book_ID_df: {len(book_ID_df)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove faulty elements from the description dataframe\n",
    "descriptions_df = descriptions_df[descriptions_df['description'].apply(lambda x: isinstance(x, str))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove \"additions\" to titles in the descriptions_df\n",
    "for i, row in tqdm(descriptions_df.iterrows()):\n",
    "    original_title = row[\"title\"]\n",
    "    new_title = re.sub(r'\\((.*)', '', original_title)\n",
    "    descriptions_df.at[i, \"title\"] = new_title.strip()\n",
    "\n",
    "# Check if it worked -> it did\n",
    "# descriptions_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lower case all titles to not have confusion in this manner\n",
    "descriptions_df['title'] = descriptions_df['title'].str.lower()\n",
    "book_ID_df['original_title'] = book_ID_df['original_title'].str.lower()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find corresponding indexes to merge the dataframes\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "titles_not_found = []\n",
    "book_df = pd.DataFrame(columns=['book_id', 'title', 'description', 'genres'])\n",
    "\n",
    "for i, row in tqdm(descriptions_df.iterrows(), total = descriptions_df.shape[0]):\n",
    "    title = row['title']\n",
    "    # Check if the title is in book_ID_df, else append it to the titles_not_found list\n",
    "    if title in book_ID_df['original_title'].values:\n",
    "        # Get all rows that have the a matching title as the current row\n",
    "        temp_df = book_ID_df[book_ID_df['original_title'] == title]\n",
    "\n",
    "        # Get the book id of the book  with the highest amount of reviews\n",
    "        book_id = temp_df['best_book_id'][temp_df['reviews_count'].idxmax()]\n",
    "        descriptions = row['description']\n",
    "        genres = row['genres']\n",
    "        book_df = book_df.append({'book_id': book_id, 'title': title, 'description': descriptions, 'genres': genres}, ignore_index=True)\n",
    "    else:\n",
    "        titles_not_found.append(title)\n",
    "\n",
    "# print the amount of elements that are not found\n",
    "print(f\"Out of the 10000 titles {len(titles_not_found)} are not found in the book_ID_df\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Somehow the same book appears multiple times, hence we drop the duplicates\n",
    "book_df.drop_duplicates(subset=['title'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# TODO - Possibly add the capacity to look through titles and determine if a shorter version could be found in the other dataset (\"harry potter and the sorcersers stone\" becoming \"harry potter and the \" and possibly finding harry potter and the philosophers stone\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess the shelves\n",
    "- Use \"book_id_map.csv\" to find the books we use (ids) and store \"new_ids\" (the ids we can use to find the relevant shelfs)\n",
    "- Drop all rows in \"goodreads_interactions.csv\" that have different ids than \"new_ids\".\n",
    "- Store this dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "book_id_map_df = pd.read_csv(DATA_PATH + \"book_id_map.csv\")\n",
    "book_df = pd.read_csv(DATA_PATH + \"book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create map from book_id to book_id_csv\n",
    "book_id_map = {book_id_map_df['book_id'][i]: book_id_map_df['book_id_csv'][i] for i in range(len(book_id_map_df))}\n",
    "\n",
    "# Change the book ID in our dataset to match the shelf dataset\n",
    "remove_list = [] # remove about 15 books that are for inexplicable reasons not in the shelf dataset\n",
    "for i in range(len(book_df)):\n",
    "    try:\n",
    "        book_df[\"book_id\"][i] = book_id_map[book_df[\"book_id\"][i]]\n",
    "    except:\n",
    "        remove_list.append(i)\n",
    "print(f\"Out of {i} books {len(remove_list)} are not in the shelf dataset and hence removed\")\n",
    "book_df.drop(remove_list, inplace=True)\n",
    "\n",
    "# Save the book_df dataframe with the index change\n",
    "book_df.to_csv(DATA_PATH + 'book_matching_ids_df.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the shelves dataframe (this takes some time and memory)\n",
    "shelves_df = pd.read_csv(DATA_PATH + \"goodreads_interactions.csv\")\n",
    "\n",
    "# Check how many books are on the shelves of ALL users combined\n",
    "print(f'There are {len(shelves_df)} books, copies counted aswell')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove books on the shelves that are not in the book_df\n",
    "shelves_df = shelves_df[shelves_df['book_id'].isin(book_df['book_id'].tolist())]\n",
    "\n",
    "# Save the new shelves_df\n",
    "shelves_df.to_csv(DATA_PATH + 'shelves_df.csv', index=False)\n",
    "\n",
    "# Check how many books are on the shelves of ALL users combined - after removal of books not in book_df\n",
    "print(f\"We have {len(shelves_df)} shelves in total, and in these there are {len(set(shelves_df['book_id'].tolist()))} unique books.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TF-IDF embeddings\n",
    "- Create TF-IDF embeddings\n",
    "- Create them for genres aswell"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports for the text analysis\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Import the book description dataframe\n",
    "book_df = pd.read_csv(DATA_PATH + \"book_matching_ids_df.csv\") # TODO help me not make a \"Unnamed: 0\" column... I want to use the book_id as index, but then it creates this column\n",
    "\n",
    "# Logorithmic scale chosen for IDF\n",
    "BASE = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Function to clean strings (from week 7)\n",
    "def clean_strings(strings):\n",
    "    \"\"\" Cleans a list of strings by removing URLs, numbers, punctuation and stop words\n",
    "\n",
    "    Args:\n",
    "    - strings: a list of strings\n",
    "\n",
    "    returns:\n",
    "    - cleaned_strings: a list of cleaned strings\n",
    "    \"\"\"\n",
    "    cleaned_strings = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for string in strings:\n",
    "        # Remove URLs\n",
    "        string = re.sub(r'http\\S+', '', string)\n",
    "\n",
    "        # Remove numbers\n",
    "        string = re.sub(r'[0-9]', '', string)\n",
    "\n",
    "        # Keep only what is not punctuation\n",
    "        string = re.sub(r'[^\\w\\s]', '', string)\n",
    "\n",
    "        # Lowercase\n",
    "        string = string.lower()\n",
    "\n",
    "        # Remove empty strings and remove stop words and moke them\n",
    "        if len(string) and string not in stop_words and type(string) == str:\n",
    "            cleaned_strings.append(string)\n",
    "\n",
    "    return cleaned_strings\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create tokens from descriptions\n",
    "# If the dataframe has not yet gotten the tokens, do it here and save it\n",
    "if not 'tokens' in book_df.columns:\n",
    "    book_df['tokens'] = None\n",
    "    for i, row in tqdm(book_df.iterrows(), total = book_df.shape[0]):\n",
    "        description = row['description']\n",
    "        # If the description is not a string, it is probably a NaN, so we set it to None\n",
    "        if type(description) == str:\n",
    "            tokens = nltk.word_tokenize(description)\n",
    "            clean_tokens = clean_strings(tokens)\n",
    "            book_df['tokens'][i] = clean_tokens # str(clean_tokens)\n",
    "\n",
    "    book_df.to_csv(DATA_PATH + 'book_matching_ids_df.csv', index=False)\n",
    "else:\n",
    "    book_df['tokens'] = book_df['tokens'].apply(ast.literal_eval)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the tf scores for each community (from the previous weeks)\n",
    "def TF_from_corpus(corpus):\n",
    "    \"\"\" Calculates the TF scores for each word in the corpus\n",
    "\n",
    "    Args:\n",
    "        corpus (list): list of lists of words/strings\n",
    "\n",
    "    Returns:\n",
    "        TF_df (pandas.DataFrame): Dataframe containing the TF scores for each word in the corpus\n",
    "    \"\"\"\n",
    "    # Create empty dictionary to keep track of word counts\n",
    "    word_counts = {}\n",
    "    n_communities = len(corpus)\n",
    "\n",
    "    # Iterate through all communities\n",
    "    for i, document in tqdm(enumerate(corpus), total=n_communities):\n",
    "        # Iterate through each word in the current sublist\n",
    "        for word in document:\n",
    "            # If the current word is not in the dictionary, add it with a list of zeros\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = [0] * n_communities\n",
    "\n",
    "            # Increment count for the current word and list index\n",
    "            word_counts[word][i] += 1\n",
    "\n",
    "    # Create pandas dataframe from the word_counts dictionary\n",
    "    TF_df = pd.DataFrame.from_dict(word_counts).transpose()\n",
    "\n",
    "    return TF_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function that takes in a TF and an IDF and computes the TF_IDF dataframe (from the previous weeks)\n",
    "def make_TF_IDF(TF_df, IDF_dict):\n",
    "    \"\"\"Multiply the TF and IDF scores to get the TF-IDF scores\n",
    "\n",
    "    Args:\n",
    "        TF_df (pandas.DataFrame): Dataframe containing the TF scores for each word in the corpus\n",
    "        IDF_dict (dict): Dictionary containing the IDF scores for each word in the corpus\n",
    "\n",
    "    Returns:\n",
    "        TF_IDF (pandas.DataFrame): Dataframe containing the TF-IDF scores for each word in the corpus\n",
    "    \"\"\"\n",
    "    # Create the TF-IDF dataframe\n",
    "    TF_IDF = pd.DataFrame(index=TF_df.index, columns=TF_df.columns)\n",
    "\n",
    "    # iterate over the index of the DataFrame\n",
    "    for word in tqdm(TF_df.index, total=TF_df.shape[0]):\n",
    "        # multiply the values by the IDF_dict value\n",
    "        TF_IDF.loc[word] = TF_df.loc[word] * IDF_dict[word]\n",
    "\n",
    "\n",
    "    return TF_IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the TF dataframe for the corpus if possible, else create it\n",
    "try:\n",
    "    TF_book_df = pd.read_csv(DATA_PATH + \"TF_book_df.csv\", index_col=0)\n",
    "except:\n",
    "    # Create it\n",
    "    TF_book_df = TF_from_corpus(book_df['tokens'])\n",
    "\n",
    "    # Rename the columns to the book_ids\n",
    "    TF_book_df.columns = book_df['book_id'].tolist()\n",
    "\n",
    "    # Save the TF dataframe\n",
    "    TF_book_df.to_csv(DATA_PATH + \"TF_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the total token count \"T_all_books\" and the IDF score for each book \"IDF_book_dict\"\n",
    "try:\n",
    "    IDF_dict = np.load(DATA_PATH + 'IDF_dict.npy', allow_pickle=True).item()\n",
    "except:\n",
    "    T_all_books = TF_book_df.apply(lambda row: (row != 0).sum(), axis=1)\n",
    "    # The log BASE is chosen when loading the libraries\n",
    "    IDF_dict = {word: np.emath.logn(BASE, len(TF_book_df.columns)/ T_all_books[word]) for word in TF_book_df.index}\n",
    "\n",
    "    np.save(DATA_PATH + 'IDF_dict.npy', IDF_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the TF-IDF scores for each book \"TF_IDF_book_df\" if it has not already been made\n",
    "try:\n",
    "    TF_IDF_book_df = pd.read_csv(DATA_PATH + \"TF_IDF_book_df.csv\", index_col=0)\n",
    "except:\n",
    "    # Create the dataframe\n",
    "    TF_IDF_book_df = make_TF_IDF(TF_book_df, IDF_dict)\n",
    "\n",
    "    # Save the dataframe\n",
    "    TF_IDF_book_df.to_csv(DATA_PATH + \"TF_IDF_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Genre TF and TF_IDF scores\n",
    "- Do this by having all books with a genre define the \"document\" for that genre\n",
    "- Then compute the \"TF_genre_df\" dataframe, by summing all books from \"TF_book_df\" from that genre\n",
    "- Here we make the decision that the IDF is the same as for the books.\n",
    "    - (Alternatively one could have weighed each book and made a new IDF score, however, this weighs a book with twice as many genres twice as large, hence we use the other option)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the set of genres\n",
    "genres = set()\n",
    "for i in book_df[\"genres\"].to_list():\n",
    "    genres = genres.union(set(ast.literal_eval(i)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try to load the TF_genres_df, else create it\n",
    "try:\n",
    "    # Load the TF_genres_df\n",
    "    TF_genres_df = pd.read_csv(DATA_PATH + \"TF_genres_df.csv\", index_col=0)\n",
    "except:\n",
    "    # For each genre, sum all TF scores for books in that genre\n",
    "    TF_genres_df = pd.DataFrame(index=TF_book_df.index, columns=genres)\n",
    "\n",
    "    TF_genres_df = TF_genres_df.fillna(0)\n",
    "\n",
    "    # Go through all books and add the TF scores to the genres of the book\n",
    "    for i, row in tqdm(book_df.iterrows()):\n",
    "        for genre in ast.literal_eval(row['genres']):\n",
    "            TF_genres_df[genre] = TF_genres_df[genre] + TF_book_df[row['book_id']]\n",
    "\n",
    "    # Save the genres_TF_df\n",
    "    TF_genres_df.to_csv(DATA_PATH + \"TF_genres_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try to load the TF_IDF_genres_df, else create it\n",
    "try:\n",
    "    # Load the TF_IDF_genres_df\n",
    "    TF_IDF_genres_df = pd.read_csv(DATA_PATH + \"TF_IDF_genres_df.csv\", index_col=0)\n",
    "except:\n",
    "    # Create the dataframe\n",
    "    TF_IDF_genres_df = make_TF_IDF(TF_genres_df, IDF_dict)\n",
    "\n",
    "    # Save the dataframe\n",
    "    TF_IDF_genres_df.to_csv(DATA_PATH + \"TF_IDF_genres_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Have not added things with # delete"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a genre for each book\n",
    "- make the inner product which each book and the genre vector (both normed)\n",
    "- let the largest inner product that the book contains be the genre of the book"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Inner product function\n",
    "def inner_product(v1, v2):\n",
    "    \"\"\"Calculates the normed inner product of two vectors\n",
    "\n",
    "    Args:\n",
    "        v1 (list): list of numbers\n",
    "        v2 (list): list of numbers\n",
    "\n",
    "    Returns:\n",
    "        inner_product (float): inner product of the two vectors (divided by the product of their norms)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Generate genre by taking inner products between each book and each genre of that book, and choosing the maximal\n",
    "def get_genres(book_df, TF_IDF_genres_df, TF_IDF_book_df):\n",
    "    # TODO: Check these args and return\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        book_df (DataFrame): DataFrame with genres for each book\n",
    "        TF_IDF_genres_df (DataFrame): DataFrame with TF_IDF each genre\n",
    "        TF_IDF_book_df (DataFrame): DataFrame with TF_IDF each book\n",
    "\n",
    "    Returns:\n",
    "        genres (dict): Dictionary with book ids as keys and the corresponding gerne that matches the best\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary to store the genre for each book\n",
    "    genres = {}\n",
    "\n",
    "    # Go through each book\n",
    "    for i, row in tqdm(book_df.iterrows()):\n",
    "        # Get the TF-IDF scores for the current book\n",
    "        book_TF_IDF = TF_IDF_book_df[row[\"book_id\"]]\n",
    "\n",
    "        best_genre = (None, 0)\n",
    "\n",
    "        # Go through each genre of the book\n",
    "        for genre in ast.literal_eval(row['genres']):\n",
    "            genre_TF_IDF = TF_IDF_genres_df[genre]\n",
    "            # print(f\"inner_product {inner_product(book_TF_IDF, genre_TF_IDF)} {genre}\") # Testing\n",
    "            if inner_product(book_TF_IDF, genre_TF_IDF) > best_genre[1]:\n",
    "                best_genre = (genre, inner_product(book_TF_IDF, genre_TF_IDF))\n",
    "        # print(best_genre) # Testing\n",
    "        # break # Testing\n",
    "        # Save the best genre\n",
    "        genres[row[\"book_id\"]] = best_genre[0]\n",
    "\n",
    "    return genres"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the genres for each book\n",
    "genres = get_genres(book_df, TF_IDF_genres_df, TF_IDF_book_df)\n",
    "\n",
    "book_df[\"top_genre\"] = book_df[\"book_id\"].map(genres)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the book_df as complete_book_df\n",
    "book_df.to_csv(DATA_PATH + \"complete_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis\n",
    "Choosing a threshold for the graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the shelf edges and store them (this takes about 12 hours)\n",
    "shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')\n",
    "# Collapse into shelves\n",
    "imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "\n",
    "# Count occurrences of each book\n",
    "book_counts = {}\n",
    "for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "    for book in shelf['book_id']:\n",
    "        if book in book_counts:\n",
    "            book_counts[book] += 1\n",
    "        else:\n",
    "            book_counts[book] = 1\n",
    "\n",
    "# Count appearances of pairs of books\n",
    "edges = {}\n",
    "for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "    if len(shelf['book_id']) > 1:\n",
    "        # Generate pairs of books\n",
    "        for i in range(len(shelf['book_id'])):\n",
    "            for j in range(i+1, len(shelf['book_id'])):\n",
    "                # Create edge\n",
    "                edge = frozenset([shelf['book_id'][i], shelf['book_id'][j]])\n",
    "                # Add edge to dictionary\n",
    "                if edge in edges:\n",
    "                    edges[edge] += 1\n",
    "                else:\n",
    "                    edges[edge] = 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# load data\n",
    "if 'shelves_df' not in locals():\n",
    "    shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of users are: 818569\n"
     ]
    }
   ],
   "source": [
    "# Implode dataframe to get books for each user in a list for each\n",
    "imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "print(f'The amount of users are: {len(imploded_df)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the full network there are 28704535 edges\n"
     ]
    }
   ],
   "source": [
    "#Get all the edges:\n",
    "edges_orginal = np.load(DATA_PATH + \"shelf_edges.npy\", allow_pickle=True).item()\n",
    "print(f'In the full network there are {len(edges_orginal)} edges')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot a histogram of the number of times each edge appears\n",
    "plt.hist(edges_orginal.values(), log=True, bins=100)\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('People who had booth books')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of books appearing together')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: That amount of edges is way to much comment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### See what thresholds would be better"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "edges_weighted = edges_orginal.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO comment on the method for weighting the edges"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 818569/818569 [00:43<00:00, 18751.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each book\n",
    "book_counts = {}\n",
    "for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "    for book in shelf['book_id']:\n",
    "        if book in book_counts:\n",
    "            book_counts[book] += 1\n",
    "        else:\n",
    "            book_counts[book] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28704535/28704535 [00:27<00:00, 1050938.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#For each edge divide the weight of it by the lowest appearence count of the two nodes it connects\n",
    "for edge in tqdm(edges_weighted):\n",
    "            edges_weighted[edge] = edges_weighted[edge] / min((book_counts[list(edge)[0]], book_counts[list(edge)[1]]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Save weighted edges\n",
    "np.save(DATA_PATH + 'shelf_edges_weighted.npy', edges_weighted)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot a histogram of the number of times each edge appears\n",
    "plt.hist(edges_weighted.values(), log=True, bins=100)\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('People who had booth books')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of books appearing together')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "while threshold <= 0.8:\n",
    "    edges_threshold = {k: v for k, v in edges_weighted.items() if v >= threshold}\n",
    "\n",
    "    nodes = {node for nodes in edges_threshold for node in nodes}\n",
    "\n",
    "    print(f'For the threshold of {np.round(threshold,2)}:')\n",
    "    print(f'The amount of edges is {len(edges_threshold)}, nodes: {len(nodes)} and the node/edges fraction is: {len(edges_threshold)*2/len(nodes)}')\n",
    "    print()\n",
    "    threshold += 0.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Todo: Snak om overvejelserne her\n",
    "# tager 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create network\n",
    "- Load dataframe with shelves\n",
    "- Implode dataframe to make shelves into edgelist\n",
    "- Calculate assortativity\n",
    "    - Genre\n",
    "    - Degrees\n",
    "- Get largest sub network\n",
    "- Create communities\n",
    "    - Color according to genre"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# Create the network\n",
    "#TODO: This an old one check if the next isn't better\n",
    "def create_network(threshold=0.5):\n",
    "    \"\"\"\n",
    "    Creates the network from the shelf edges.\n",
    "    If the edges have already been created, they are loaded.\n",
    "    Otherwise, they are created and stored. The edges are filtered by the threshhold. The network is returned.\n",
    "\n",
    "    Args:\n",
    "        threshhold (int): The minimum number of times an edge must appear to be included in the network.\n",
    "\n",
    "    Returns:\n",
    "        G (networkx.Graph): The network of book edges.\n",
    "    \"\"\"\n",
    "    # If the edges have already been created, load them\n",
    "    try:\n",
    "        # Load the shelf edges\n",
    "        edges = np.load(DATA_PATH + 'edges_weighted.npy', allow_pickle=True).item() # (takes about 10 minutes)\n",
    "        print('Loaded weighted edges')\n",
    "    except:\n",
    "\n",
    "        # Create the shelf edges and store them (this takes about 12 hours)\n",
    "        shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')\n",
    "        # Collapse into shelves\n",
    "        imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "\n",
    "        # Count occurrences of each book\n",
    "        book_counts = {}\n",
    "        for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "            for book in shelf['book_id']:\n",
    "                if book in book_counts:\n",
    "                    book_counts[book] += 1\n",
    "                else:\n",
    "                    book_counts[book] = 1\n",
    "\n",
    "        # Count appearances of pairs of books\n",
    "        edges = {}\n",
    "        for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "            if len(shelf['book_id']) > 1:\n",
    "                # Generate pairs of books\n",
    "                for i in range(len(shelf['book_id'])):\n",
    "                    for j in range(i+1, len(shelf['book_id'])):\n",
    "                        # Create edge\n",
    "                        edge = frozenset([shelf['book_id'][i], shelf['book_id'][j]])\n",
    "                        # Add edge to dictionary\n",
    "                        if edge in edges:\n",
    "                            edges[edge] += 1\n",
    "                        else:\n",
    "                            edges[edge] = 1\n",
    "\n",
    "        # Weigh edges by dividing the count by the lowest amount of appearances between the two books\n",
    "        for edge in edges:\n",
    "            edges[edge] = edges[edge] / min(book_counts[list(edge)[0]], book_counts[list(edge)[1]])\n",
    "\n",
    "        # Store the edges\n",
    "        np.save(DATA_PATH + 'shelf_edges.npy', edges)\n",
    "\n",
    "    print('Starting to create edges')\n",
    "\n",
    "    # Remove edges that are below the threshhold\n",
    "    edges = {k: v for k, v in tqdm(edges.items()) if v >= threshold}\n",
    "\n",
    "    # Create the network\n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(([(u, v, w) for (u, v), w in edges.items()]))\n",
    "\n",
    "    return G\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\ndef create_network(threshold=0.2):\\n\\n    Creates the network from the shelf edges.\\n    If the edges have already been created, they are loaded.\\n    Otherwise, they are created and stored. The edges are filtered by the threshold. The network is returned.\\n\\n    Args:\\n        threshold (int): The minimum number of times an edge must appear to be included in the network.\\n\\n    Returns:\\n        G (networkx.Graph): The network of book edges.\\n\\n    # If the edges have already been created, load them\\n    try:\\n        # Load the shelf edges\\n        edges = np.load(DATA_PATH + 'shelf_edges.npy', allow_pickle=True).item() # (takes about 10 minutes)\\n    except:\\n        # Create the shelf edges and store them (this takes about 12 hours)\\n        shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')\\n        # Collapse into shelves\\n        imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\\n\\n        # Count occurrences of each book\\n        book_counts = {}\\n        for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\\n            for book in shelf['book_id']:\\n                if book in book_counts:\\n                    book_counts[book] += 1\\n                else:\\n                    book_counts[book] = 1\\n\\n\\n\\n    # Load the edges\\n    edges = np.load(DATA_PATH + 'shelf_edges.npy', allow_pickle=True)\\n\\n    # Remove edges that are below the threshold\\n    edges = {k: v for k, v in edges.items() if v >= threshold}\\n\\n    # Create the network\\n    G = nx.Graph()\\n    G.add_edges_from(edges.keys())\\n\\n    return G\\n\""
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def create_network(threshold=0.2):\n",
    "\n",
    "    Creates the network from the shelf edges.\n",
    "    If the edges have already been created, they are loaded.\n",
    "    Otherwise, they are created and stored. The edges are filtered by the threshold. The network is returned.\n",
    "\n",
    "    Args:\n",
    "        threshold (int): The minimum number of times an edge must appear to be included in the network.\n",
    "\n",
    "    Returns:\n",
    "        G (networkx.Graph): The network of book edges.\n",
    "\n",
    "    # If the edges have already been created, load them\n",
    "    try:\n",
    "        # Load the shelf edges\n",
    "        edges = np.load(DATA_PATH + 'shelf_edges.npy', allow_pickle=True).item() # (takes about 10 minutes)\n",
    "    except:\n",
    "        # Create the shelf edges and store them (this takes about 12 hours)\n",
    "        shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')\n",
    "        # Collapse into shelves\n",
    "        imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "\n",
    "        # Count occurrences of each book\n",
    "        book_counts = {}\n",
    "        for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "            for book in shelf['book_id']:\n",
    "                if book in book_counts:\n",
    "                    book_counts[book] += 1\n",
    "                else:\n",
    "                    book_counts[book] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # Load the edges\n",
    "    edges = np.load(DATA_PATH + 'shelf_edges.npy', allow_pickle=True)\n",
    "\n",
    "    # Remove edges that are below the threshold\n",
    "    edges = {k: v for k, v in edges.items() if v >= threshold}\n",
    "\n",
    "    # Create the network\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges.keys())\n",
    "\n",
    "    return G\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "if not \"complete_book_df\" in locals():\n",
    "    complete_book_df = pd.read_csv(DATA_PATH + \"complete_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# Give nodes attributes\n",
    "def make_attributes(df, graph):\n",
    "    #TODO: args and returns\n",
    "    # make a dictionary with attributes for each node\n",
    "    book_attributes = dict()\n",
    "    for i, book in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        node = book['book_id']\n",
    "        if not node in graph.nodes():\n",
    "            continue\n",
    "        book_attributes[node] = dict()\n",
    "        top_genre = book['top_genre']\n",
    "        title = book['title']\n",
    "        genres = eval(book['genres'])\n",
    "\n",
    "        book_attributes[node]['title'] = title\n",
    "        book_attributes[node]['genres'] = genres\n",
    "        book_attributes[node]['top_genre'] = top_genre\n",
    "    return book_attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weighted edges\n",
      "Starting to create edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28704535/28704535 [00:03<00:00, 7498320.51it/s]\n",
      "100%|██████████| 7676/7676 [00:00<00:00, 18245.82it/s]\n"
     ]
    }
   ],
   "source": [
    "folder_path = DATA_PATH\n",
    "file_name = \"shelves_network_05.pickle\"\n",
    "\n",
    "if not os.path.exists(os.path.join(folder_path, file_name)):\n",
    "    # Make the network\n",
    "    shelf_graph = create_network(0.5)\n",
    "\n",
    "    #Give nodes attributes\n",
    "    shelf_attribute_dict = make_attributes(complete_book_df, shelf_graph)\n",
    "    nx.set_node_attributes(shelf_graph, shelf_attribute_dict)\n",
    "\n",
    "    # Save the network\n",
    "    with open(DATA_PATH + file_name, 'wb') as f:\n",
    "        pickle.dump(shelf_graph, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 7539 nodes and 109627 edges\n"
     ]
    }
   ],
   "source": [
    "print(shelf_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create NLP network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "# Used to make all node ids ints\n",
    "def make_all_nodes_ints(graph):\n",
    "    #TODO: args and returns\n",
    "\n",
    "    # Check if first id is a string, in which case, don't do anyting\n",
    "    if isinstance(list(graph.nodes())[0], int):\n",
    "            return graph\n",
    "\n",
    "    # Make a mapping so each node can be renamed into what it was, but a string instead\n",
    "    node_map = {}\n",
    "    for node in graph.nodes():\n",
    "        node_map[node] = int(node)\n",
    "    str_graph = nx.relabel_nodes(graph, node_map)\n",
    "\n",
    "    return str_graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "if 'shelf_graph' not in locals():\n",
    "    with open(DATA_PATH + 'shelves_network_05.pickle', 'rb') as f:\n",
    "        shelf_graph = pickle.load(f)\n",
    "        shelf_graph = make_all_nodes_ints(shelf_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "#TODO: Comment on how we make the NLP network (Inner products)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[99], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load the TF_IDF values for each book\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m tf_idf_book \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA_PATH\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTF_IDF_book_df.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m book_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(DATA_PATH \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete_book_df.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    947\u001B[0m )\n\u001B[0;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    610\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 611\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1771\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1773\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1774\u001B[0m     (\n\u001B[0;32m   1775\u001B[0m         index,\n\u001B[0;32m   1776\u001B[0m         columns,\n\u001B[0;32m   1777\u001B[0m         col_dict,\n\u001B[1;32m-> 1778\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1779\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1780\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1782\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 230\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader\u001B[38;5;241m.\u001B[39mread_low_memory(nrows)\n\u001B[0;32m    231\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    232\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1037\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1083\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1158\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1478\u001B[0m, in \u001B[0;36mis_extension_array_dtype\u001B[1;34m(arr_or_dtype)\u001B[0m\n\u001B[0;32m   1433\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_extension_array_dtype\u001B[39m(arr_or_dtype) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m   1434\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1435\u001B[0m \u001B[38;5;124;03m    Check if an object is a pandas extension array type.\u001B[39;00m\n\u001B[0;32m   1436\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1476\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n\u001B[0;32m   1477\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1478\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(arr_or_dtype, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, arr_or_dtype)\n\u001B[0;32m   1479\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, ExtensionDtype):\n\u001B[0;32m   1480\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Load the TF_IDF values for each book\n",
    "tf_idf_book = pd.read_csv(DATA_PATH + \"TF_IDF_book_df.csv\", index_col=0)\n",
    "book_df = pd.read_csv(DATA_PATH + \"complete_book_df.csv\", index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# Inner product function\n",
    "def inner_product(v1, v2):\n",
    "    \"\"\"Calculates the normed inner product of two vectors\n",
    "\n",
    "    Args:\n",
    "        v1 (list): list of numbers\n",
    "        v2 (list): list of numbers\n",
    "\n",
    "    Returns:\n",
    "        inner_product (float): inner product of the two vectors (divided by the product of their norms)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges were loaded\n"
     ]
    }
   ],
   "source": [
    "# Create the inner product between every book\n",
    "try:\n",
    "    NLP_edges = np.load(DATA_PATH + 'NLP_edges.npy', allow_pickle=True).item()\n",
    "    print('Edges were loaded')\n",
    "except:\n",
    "    similarities = {}\n",
    "    for i in tqdm(range(len(book_df))):\n",
    "        for j in range(i+1, len(book_df)):\n",
    "            similarities[(book_df.iloc[i][\"book_id\"], book_df.iloc[j][\"book_id\"])] = inner_product(tf_idf_book[str(book_df.iloc[i][\"book_id\"])], tf_idf_book[str(book_df.iloc[j][\"book_id\"])])\n",
    "\n",
    "    # Save the edges\n",
    "    np.save(DATA_PATH + 'NLP_edges.npy', similarities)\n",
    "    NLP_edges = similarities\n",
    "    print('Edges were saved')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The threshold to have the same number of edges in the NLP as the shelf graph is: 0.056\n"
     ]
    }
   ],
   "source": [
    "# Find the value that separates the top 256679 larges edges from the rest\n",
    "threshold = sorted(NLP_edges.values(), reverse=True)[shelf_graph.number_of_edges()]\n",
    "print(f'The threshold to have the same number of edges in the NLP as the shelf graph is: {np.round(threshold,3)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "NLP_edges = {k: v for k, v in NLP_edges.items() if v >= threshold}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 7661 nodes and 109628 edges\n"
     ]
    }
   ],
   "source": [
    "NLP_graph = nx.Graph()\n",
    "NLP_graph.add_weighted_edges_from(([(u, v, w) for (u, v), w in NLP_edges.items()]))\n",
    "print(NLP_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "if not \"complete_book_df\" in locals():\n",
    "    complete_book_df = pd.read_csv(DATA_PATH + \"complete_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7676/7676 [00:00<00:00, 22021.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Give nodes attributes\n",
    "NLP_attribute_dict = make_attributes(complete_book_df, NLP_graph)\n",
    "nx.set_node_attributes(NLP_graph, NLP_attribute_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# Save NLP_graph\n",
    "folder_path = DATA_PATH\n",
    "file_name = \"NLP_network_05.pickle\"\n",
    "\n",
    "if not os.path.exists(os.path.join(folder_path, file_name)):\n",
    "    # Save the network\n",
    "    with open(DATA_PATH + file_name, 'wb') as f:\n",
    "        pickle.dump(NLP_graph, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Communities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "import community"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# Function that applies the Louvain algorithm to a grap, and returns communities\n",
    "def louvain_communities(graph):\n",
    "    # Use the louvain method to find communities\n",
    "    partition = community.best_partition(graph) # community function that uses Louvain-algorithm\n",
    "\n",
    "    # Reformat the partitioning\n",
    "    communities = {}\n",
    "    for node, community_id in partition.items():\n",
    "        if community_id in communities:\n",
    "            communities[community_id].append(node)\n",
    "        else:\n",
    "            communities[community_id] = [node]\n",
    "\n",
    "    return list(communities.values())\n",
    "\n",
    "def louvain_communities_faster(graph):\n",
    "    # Use the louvain method to find communities\n",
    "    partition = community.best_partition(graph) # community function that uses Louvain-algorithm\n",
    "\n",
    "    # Reformat the partitioning\n",
    "    communities = {}\n",
    "    for node, community_id in tqdm(partition.items()):\n",
    "        communities.setdefault(community_id, []).append(node)\n",
    "\n",
    "    return list(communities.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7539/7539 [00:00<00:00, 3755000.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save shelf_louvain_05\n",
    "folder_path = DATA_PATH\n",
    "file_name = \"shelf_louvain_051.pickle\"\n",
    "\n",
    "if not os.path.exists(os.path.join(folder_path, file_name)):\n",
    "    # Save the communities\n",
    "    shelf_louvain = louvain_communities_faster(shelf_graph)\n",
    "    with open(DATA_PATH + 'shelf_louvain_05.pickle', 'wb') as f:\n",
    "        pickle.dump(shelf_louvain, f)\n",
    "else:\n",
    "    # Load the communities\n",
    "    with open(DATA_PATH + 'shelf_louvain_05.pickle', 'rb') as f:\n",
    "        shelf_louvain = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7661/7661 [00:00<00:00, 3058205.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save NLP_graph\n",
    "folder_path = DATA_PATH\n",
    "file_name = \"NLP_louvain_05.pickle\"\n",
    "\n",
    "if not os.path.exists(os.path.join(folder_path, file_name)):\n",
    "    # Save the network\n",
    "    NLP_louvain = louvain_communities_faster(NLP_graph)\n",
    "    with open(DATA_PATH + file_name, 'wb') as f:\n",
    "        pickle.dump(NLP_louvain, f)\n",
    "else:\n",
    "    with open(DATA_PATH + file_name, 'rb') as f:\n",
    "        NLP_louvain = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modularity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "#Modularity function\n",
    "def modularity(G, communities):\n",
    "    \"\"\"\n",
    "    Compute the modularity for a communities of a graph.\n",
    "\n",
    "    Parameters:\n",
    "        - G: the NetworkX graph\n",
    "        - communities (list): a list of communities, where each community is a list of node IDs\n",
    "\n",
    "    Returns:\n",
    "        - M: the modularity value for the graph\n",
    "    \"\"\"\n",
    "\n",
    "    total_links = G.number_of_edges()\n",
    "    num_of_communities = len(communities)\n",
    "    links_in_community = {community: 0 for community in range(num_of_communities)}\n",
    "    tot_degree = {community: 0 for community in range(num_of_communities)}\n",
    "\n",
    "    # Dictionary to map node IDs to community IDs\n",
    "    communities_lookup = {node: community for community, nodes in enumerate(communities) for node in nodes}\n",
    "\n",
    "    # Count the number of links within each community and the total degree of each community\n",
    "    for (node_1, node_2) in G.edges():\n",
    "        node_1_community = communities_lookup[node_1]\n",
    "        node_2_community = communities_lookup[node_2]\n",
    "\n",
    "        # Since we go through all edges, we also get the degree of all nodes\n",
    "        tot_degree[node_1_community] += 1\n",
    "        tot_degree[node_2_community] += 1\n",
    "\n",
    "        # If the nodes are in the same community, add one to that community's link count\n",
    "        if node_1_community == node_2_community:\n",
    "            links_in_community[node_1_community] += 1\n",
    "\n",
    "    # Compute the modularity\n",
    "    M = 0.0\n",
    "    for i in range(num_of_communities):\n",
    "        M += (links_in_community[i] / total_links) - ((tot_degree[i] / (2 * total_links)) ** 2)\n",
    "\n",
    "    return M\n",
    "\n",
    "def modularity_faster(G, communities):\n",
    "    \"\"\"\n",
    "    Compute the modularity for a communities of a graph.\n",
    "\n",
    "    Parameters:\n",
    "        - G: the NetworkX graph\n",
    "        - communities (list): a list of communities, where each community is a list of node IDs\n",
    "\n",
    "    Returns:\n",
    "        - M: the modularity value for the graph\n",
    "    \"\"\"\n",
    "\n",
    "    total_links = G.number_of_edges()\n",
    "    num_of_communities = len(communities)\n",
    "    links_in_community = [0] * num_of_communities\n",
    "    tot_degree = [0] * num_of_communities\n",
    "    nodes_in_community = [set(c) for c in communities]\n",
    "\n",
    "    # Compute the total degree of nodes in each community\n",
    "    for node, community in enumerate(nodes_in_community):\n",
    "        community_edges = G.subgraph(community).edges()\n",
    "        community_degree = sum([G.degree(n) for n in community])\n",
    "        tot_degree[node] = community_degree\n",
    "        for n1, n2 in community_edges:\n",
    "            if n1 in community and n2 in community:\n",
    "                links_in_community[node] += 1\n",
    "\n",
    "    # Compute the modularity\n",
    "    M = 0.0\n",
    "    for i in range(num_of_communities):\n",
    "        M += (links_in_community[i] / total_links) - ((tot_degree[i] / (2 * total_links)) ** 2)\n",
    "\n",
    "    return M\n",
    "\n",
    "def modularity_nx(G, communities):\n",
    "    \"\"\"\n",
    "    Compute the modularity for a communities of a graph.\n",
    "\n",
    "    Parameters:\n",
    "        - G: the NetworkX graph\n",
    "        - communities (list): a list of communities, where each community is a list of node IDs\n",
    "\n",
    "    Returns:\n",
    "        - M: the modularity value for the graph\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the louvain method to find communities\n",
    "    partition = {node: community_id for community_id, nodes in enumerate(communities) for node in nodes}\n",
    "\n",
    "    # Compute the modularity\n",
    "    M = community.modularity(partition, G)\n",
    "\n",
    "    return M\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modularity of shelf graph is: 0.4382068319772454\n"
     ]
    }
   ],
   "source": [
    "shelf_modularity = modularity_nx(shelf_graph,shelf_louvain)\n",
    "print('The modularity of shelf graph is:', shelf_modularity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modularity of NLP graph is: 0.3795850839024229\n"
     ]
    }
   ],
   "source": [
    "NLP_modularity = modularity_nx(NLP_graph,NLP_louvain)\n",
    "print('The modularity of NLP graph is:', NLP_modularity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "#Random Edge swap function\n",
    "\n",
    "def edge_swap(G, N):\n",
    "    \"\"\"\n",
    "    create a copy of G with N swapped edges\n",
    "\n",
    "    Parameters:\n",
    "        - G: the given graph\n",
    "        - N: how many edges we want swapped\n",
    "\n",
    "    Returns:\n",
    "        - G_copy: a copy of graph g with N edges swapped at random\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of the original network, to not change the original\n",
    "    G = copy.deepcopy(G)\n",
    "\n",
    "    # Perform at least N edge swaps\n",
    "    edges_swapped = 0\n",
    "    while edges_swapped <= N:\n",
    "        # Choose two random edges\n",
    "        edges = random.sample(G.edges(), 2)\n",
    "        (u, v) = edges[0]\n",
    "        (x, y) = edges[1]\n",
    "        # Check that we will not link a node to itself\n",
    "        # We believe that there was an error in the instructions\n",
    "        # That it is not meant to say u != v, as this is trivial and can't happen\n",
    "        # Instead it is vital to check that u != y such that we don't link u to itself\n",
    "        if u != y and v != x:\n",
    "            # Check if the edges (u,y) and (x,v) exist\n",
    "            if not G.has_edge(u, y) and not G.has_edge(x, v):\n",
    "                # Swap the edges\n",
    "                G.remove_edge(u, v)\n",
    "                G.remove_edge(x, y)\n",
    "                G.add_edge(u, y)\n",
    "                G.add_edge(x, v)\n",
    "\n",
    "                #Counter for number of swaps\n",
    "                edges_swapped += 1\n",
    "\n",
    "    return G"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 7539 nodes and 109627 edges\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Population must be a sequence.  For dicts or sets, use sorted(d).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[119], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 2\u001B[0m     shelf_modularity \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA_PATH\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshelf_modularity.npy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_pickle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m#Set seed in the interest of repeatability\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 405\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28mopen\u001B[39m(os_fspath(file), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m    406\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/shelf_modularity.npy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[119], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m#The loop that produces a thousand edge swapped datasets, and appends them to rand_modularities\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m#Here we use the previously defined modularity and edge_swap funtions\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m)):\n\u001B[1;32m---> 17\u001B[0m     rand_giant \u001B[38;5;241m=\u001B[39m \u001B[43medge_swap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshelf_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshelf_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medges\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m     shelf_modularity\u001B[38;5;241m.\u001B[39mappend(modularity(rand_giant, shelf_louvain))\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Save the date\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[73], line 22\u001B[0m, in \u001B[0;36medge_swap\u001B[1;34m(G, N)\u001B[0m\n\u001B[0;32m     19\u001B[0m edges_swapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m edges_swapped \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m N:\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# Choose two random edges\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m     edges \u001B[38;5;241m=\u001B[39m \u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medges\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     (u, v) \u001B[38;5;241m=\u001B[39m edges[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     24\u001B[0m     (x, y) \u001B[38;5;241m=\u001B[39m edges[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\random.py:439\u001B[0m, in \u001B[0;36mRandom.sample\u001B[1;34m(self, population, k, counts)\u001B[0m\n\u001B[0;32m    415\u001B[0m \u001B[38;5;66;03m# Sampling without replacement entails tracking either potential\u001B[39;00m\n\u001B[0;32m    416\u001B[0m \u001B[38;5;66;03m# selections (the pool) in a list or previous selections in a set.\u001B[39;00m\n\u001B[0;32m    417\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    435\u001B[0m \u001B[38;5;66;03m# too many calls to _randbelow(), making them slower and\u001B[39;00m\n\u001B[0;32m    436\u001B[0m \u001B[38;5;66;03m# causing them to eat more entropy than necessary.\u001B[39;00m\n\u001B[0;32m    438\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(population, _Sequence):\n\u001B[1;32m--> 439\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPopulation must be a sequence.  \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    440\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFor dicts or sets, use sorted(d).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    441\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(population)\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m counts \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mTypeError\u001B[0m: Population must be a sequence.  For dicts or sets, use sorted(d)."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    shelf_modularity = np.load(DATA_PATH + \"shelf_modularity.npy\", allow_pickle=True)\n",
    "except:\n",
    "    #Set seed in the interest of repeatability\n",
    "    random.seed(69420)\n",
    "    #A list that will contain a thousand randomized modularities\n",
    "    shelf_modularity = []\n",
    "\n",
    "    # Load the louvain groups if not in memory\n",
    "    if 'shelf_louvain' not in locals():\n",
    "        with open('shelf_louvain.pickle', 'rb') as f:\n",
    "            shelf_louvain = pickle.load(f)\n",
    "\n",
    "    #The loop that produces a thousand edge swapped datasets, and appends them to rand_modularities\n",
    "    #Here we use the previously defined modularity and edge_swap functions\n",
    "    for i in tqdm(range(100)):\n",
    "        shelf_rand = edge_swap(shelf_graph, len(shelf_graph.edges()))\n",
    "        shelf_modularity.append(modularity(shelf_rand, shelf_louvain))\n",
    "\n",
    "    # Save the date\n",
    "    np.save(DATA_PATH + \"shelf_modularity.npy\", shelf_modularity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Matching attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "if 'shelf_graph' not in locals():\n",
    "    shelf_graph = nx.read_graphml(DATA_PATH + 'shelves_graph_04.graphml') #TODO ændres til 05\n",
    "    shelf_graph = make_all_nodes_ints(shelf_graph)\n",
    "\n",
    "if 'NLP_graph' not in locals():\n",
    "    NLP_graph = nx.read_graphml(DATA_PATH + 'NLP_graph_04.graphml') #TODO ændres til 05\n",
    "    NLP_graph = make_all_nodes_ints(NLP_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# Function to calculate the fraction of neighbors that have the same attribute value\n",
    "def get_matching_att(graph, att):\n",
    "    \"\"\"\n",
    "    A function that calculates the fraction of neighbors that have the same attribute value\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The graph to be analyzed\n",
    "        att (str): The attribute to be analyzed\n",
    "\n",
    "    Returns:\n",
    "        match_frac (dict): A dictionary with the fraction of neighbors that have the same attribute value\n",
    "    \"\"\"\n",
    "    match_frac = {}\n",
    "    for node in tqdm(graph.nodes()):\n",
    "        counter = 0\n",
    "        neighbors = len(list(graph.neighbors(node)))\n",
    "\n",
    "        if neighbors == 0: match_frac[node] = 0 # Have to check this, otherwise division by zero\n",
    "\n",
    "        else:\n",
    "            for neighbor in graph.neighbors(node):\n",
    "                # Check if the attribute value is the same for the node and the neighbor\n",
    "                if nx.get_node_attributes(graph, att)[neighbor] == nx.get_node_attributes(graph, att)[node]:\n",
    "                    counter += 1\n",
    "\n",
    "            match_frac[node] = counter / neighbors\n",
    "\n",
    "    return match_frac"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(shelves) Average across all nodes 0.22811813442165993\n",
      "(NLP) Average across all nodes 0.1997208605593006\n"
     ]
    }
   ],
   "source": [
    "# Get the average fraction of neighbors that have the same attribute value\n",
    "top_field_fracs_shelves = get_matching_att(shelf_graph, 'top_genre')\n",
    "top_field_fracs_NLP = get_matching_att(NLP_graph, 'top_genre')\n",
    "\n",
    "print(f\"(shelves) Average across all nodes {np.mean(list(top_field_fracs_shelves.values()))}\")\n",
    "print(f\"(NLP) Average across all nodes {np.mean(list(top_field_fracs_NLP.values()))}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Create a new graph, with the same nodes and edges, but where the association between nodes and field is shuffled. Compute the measure above for this randomized graph."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# Function that shuffles the attribute values of a graph\n",
    "def shuffle_node_att(graph, att):\n",
    "    \"\"\"\n",
    "    A function that shuffles the attribute values of a graph\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The graph to be analyzed\n",
    "        att (str): The attribute to be analyzed\n",
    "\n",
    "    Returns:\n",
    "        graph (nx.Graph): The graph with shuffled attributes\n",
    "    \"\"\"\n",
    "    # Make dictionary with shuffled attributes\n",
    "    shuffled_atts = nx.get_node_attributes(graph, att)\n",
    "    temp = list(shuffled_atts.values())\n",
    "    random.shuffle(temp)\n",
    "    new_atts = dict(zip(shuffled_atts, temp))\n",
    "\n",
    "    # Set the shuffled attributes\n",
    "    nx.set_node_attributes(graph, new_atts, att)\n",
    "\n",
    "    return graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Shelves) When the fields are randomly assigned, on average an genre has 0.22811813442165993 of their neighbors in the same genre\n",
      "(NLP) When the fields are randomly assigned, on average an genre has 0.04969976529917715 of their neighbors in the same genre\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the graph and shuffle the attribute values\n",
    "shelf_giant_copy = copy.deepcopy(shelf_graph)\n",
    "shelf_giant_shuffled = shuffle_node_att(shelf_graph, 'top_genre')\n",
    "\n",
    "NLP_giant_copy = copy.deepcopy(NLP_graph)\n",
    "NLP_giant_shuffled = shuffle_node_att(NLP_graph, 'top_genre')\n",
    "\n",
    "print(f\"(Shelves) When the fields are randomly assigned, on average an genre has {np.mean(list(get_matching_att(shelf_giant_copy, 'top_genre').values()))} of their neighbors in the same genre\")\n",
    "print(f\"(NLP) When the fields are randomly assigned, on average an genre has {np.mean(list(get_matching_att(NLP_giant_shuffled, 'top_genre').values()))} of their neighbors in the same genre\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import netwulf\n",
    "import ast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "shelf_graph = nx.read_graphml(DATA_PATH + 'shelves_graph_04.graphml')\n",
    "shelf_graph = make_all_nodes_ints(shelf_graph)\n",
    "\n",
    "NLP_graph = nx.read_graphml(DATA_PATH + 'NLP_graph_04.graphml')\n",
    "NLP_graph = make_all_nodes_ints(NLP_graph)\n",
    "\n",
    "#Load communities\n",
    "shelf_louvain = np.load(DATA_PATH + 'shelves_communities_04.npy', allow_pickle = True)\n",
    "NLP_louvain = np.load(DATA_PATH + 'NLP_communities_04.npy', allow_pickle= True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "if not \"complete_book_df\" in locals():\n",
    "    complete_book_df = pd.read_csv(DATA_PATH + \"complete_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def make_attributes(df, graph):\n",
    "    #TODO: args and returns\n",
    "    # make a dictionary with attributes for each node\n",
    "    book_attributes = dict()\n",
    "    for i, book in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        node = book['book_id']\n",
    "        if not node in graph.nodes():\n",
    "            continue\n",
    "        book_attributes[node] = dict()\n",
    "        top_genre = book['top_genre']\n",
    "        title = book['title']\n",
    "        genres = ast.literal_eval(book['genres'])\n",
    "\n",
    "        book_attributes[node]['title'] = title\n",
    "        book_attributes[node]['genres'] = genres\n",
    "        book_attributes[node]['top_genre'] = top_genre\n",
    "    return book_attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7676/7676 [00:00<00:00, 21558.65it/s]\n",
      "100%|██████████| 7676/7676 [00:00<00:00, 23929.70it/s]\n"
     ]
    }
   ],
   "source": [
    "shelf_attribute_dict = make_attributes(complete_book_df, shelf_graph)\n",
    "NLP_attribute_dict = make_attributes(complete_book_df, NLP_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "nx.set_node_attributes(shelf_graph, shelf_attribute_dict)\n",
    "nx.set_node_attributes(NLP_graph, NLP_attribute_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "# List of colors to use for the visualization\n",
    "\n",
    "\n",
    "# Use netwulf to visualize the communities\n",
    "def visualize_communities(graph, communities):\n",
    "    colors = list(mcolors.CSS4_COLORS.values())[20:len(communities) + 20]\n",
    "    # Create a dictionary that maps nodes to communities\n",
    "    node_to_community = {}\n",
    "    for i, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            node = str(node)\n",
    "            node_to_community[node] = i\n",
    "\n",
    "    # Add a color attribute to the nodes\n",
    "    for node in graph.nodes:\n",
    "        # Add the community number as a node attribute\n",
    "        graph.nodes[node][\"color\"] = colors[node_to_community[node]]\n",
    "\n",
    "    # Visualize the graph with netwulf\n",
    "    netwulf.visualize(graph, config={\"Node color\": \"color\",\n",
    "                                    'node_size': 50,\n",
    "                                    'link_width': 0.1,\n",
    "                                    'link_alpha': 0.01,\n",
    "                                    'node_size_variation': 0.3,\n",
    "                                    'zoom': 0.65,\n",
    "                                    'scale_node_size_by_strength': True,})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "visualize_communities(shelf_graph, shelf_louvain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "visualize_communities(NLP_graph, NLP_louvain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Colors for genre"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def create_genre_dict(graph):\n",
    "    genre_dict = {}\n",
    "    for node in tqdm(graph.nodes):\n",
    "        genre = nx.get_node_attributes(graph, 'top_genre')[node]\n",
    "        try:\n",
    "            genre_dict[genre].append(node)\n",
    "        except:\n",
    "            genre_dict[genre] = [node]\n",
    "    return genre_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7665/7665 [00:12<00:00, 612.85it/s]\n",
      "100%|██████████| 7671/7671 [00:12<00:00, 611.70it/s]\n"
     ]
    }
   ],
   "source": [
    "shelf_genre_dict = create_genre_dict(shelf_graph)\n",
    "NLP_genre_dict = create_genre_dict(NLP_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Use netwulf to visualize the genres\n",
    "def visualize_genres(graph, genre_dict):\n",
    "    colors = list(mcolors.CSS4_COLORS.values())[20:len(genre_dict.keys())+20]\n",
    "    # Create a dictionary that maps nodes to genre\n",
    "    node_to_genre = {}\n",
    "    for i, genre in enumerate(genre_dict.keys()):\n",
    "        for node in genre_dict[genre]:\n",
    "            node_to_genre[node] = i\n",
    "\n",
    "\n",
    "\n",
    "    # Add a color attribute to the nodes\n",
    "    for node in graph.nodes:\n",
    "        # Add the community number as a node attribute\n",
    "        graph.nodes[node][\"color\"] = colors[node_to_genre[node]]\n",
    "\n",
    "    # Visualize the graph with netwulf\n",
    "    netwulf.visualize(graph, config={\"Node color\": \"color\",\n",
    "                                    'node_size': 50,\n",
    "                                    'link_width': 0.1,\n",
    "                                    'link_alpha': 0.01,\n",
    "                                    'node_size_variation': 0.3,\n",
    "                                    'zoom': 0.65,\n",
    "                                    'scale_node_size_by_strength': True,})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "visualize_genres(shelf_graph, shelf_genre_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "visualize_genres(NLP_graph, NLP_genre_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate wordclouds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Find top three books for each community according to degree\n",
    "def get_top_3_books(graph_type, louvain_groups):\n",
    "    top_3_books = {}\n",
    "    # Store the top 3 books by degree for all communities\n",
    "    for i, community in enumerate(louvain_groups):\n",
    "\n",
    "        # Get the top 3 books by degree\n",
    "        sorted_dict = sorted(dict(graph_type.degree(community)).items(), key=lambda x: x[1], reverse=True)\n",
    "        names = nx.get_node_attributes(graph_type, \"title\")\n",
    "        top_3_keys = [k for k, v in sorted_dict[:3]] # Get the ID\n",
    "        top_3_names = [names[k] for k in top_3_keys] # get the name\n",
    "\n",
    "        top_3_books[i] = [(top_3_keys[j], top_3_names[j]) for j in range(len(top_3_keys))]\n",
    "        return top_3_books"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shelf graph\n",
      "**********\n",
      "1. largest community 1: \n",
      "Book: the hunger games, degree: 7312\n",
      "Book: to kill a mockingbird, degree: 6527\n",
      "Book: catching fire, degree: 5683\n",
      "\n",
      "2. largest community 0: \n",
      "Book: harry potter and the chamber of secrets, degree: 5694\n",
      "Book: harry potter and the prisoner of azkaban, degree: 5530\n",
      "Book: harry potter and the deathly hallows, degree: 5394\n",
      "\n",
      "3. largest community 5: \n",
      "Book: eclipse, degree: 2309\n",
      "Book: breaking dawn, degree: 2232\n",
      "Book: city of bones, degree: 1947\n",
      "\n",
      "4. largest community 3: \n",
      "Book: angels & demons, degree: 1024\n",
      "Book: a time to kill, degree: 151\n",
      "Book: along came a spider, degree: 145\n",
      "\n",
      "5. largest community 2: \n",
      "Book: the riddle, degree: 1384\n",
      "Book: a wrinkle in time, degree: 771\n",
      "Book: charlotte's web, degree: 657\n",
      "\n",
      "6. largest community 9: \n",
      "Book: fifty shades of grey, degree: 890\n",
      "Book: beautiful disaster, degree: 496\n",
      "Book: fifty shades darker, degree: 399\n",
      "\n",
      "7. largest community 6: \n",
      "Book: dark lover, degree: 498\n",
      "Book: dead until dark, degree: 405\n",
      "Book: halfway to the grave, degree: 291\n",
      "\n",
      "8. largest community 4: \n",
      "Book: the shining, degree: 322\n",
      "Book: the stand, degree: 259\n",
      "Book: dark visions, degree: 258\n",
      "\n",
      "9. largest community 7: \n",
      "Book: the tipping point: how little things can make a big difference, degree: 101\n",
      "Book: outliers: the story of success, degree: 96\n",
      "Book: freakonomics: a rogue economist explores the hidden side of everything, degree: 72\n",
      "\n",
      "NLP graph\n",
      "**********\n",
      "1. largest community 4: \n",
      "Book: abandon, degree: 667\n",
      "Book: fueled, degree: 535\n",
      "Book: wait for it, degree: 454\n",
      "\n",
      "2. largest community 8: \n",
      "Book: the greatest generation, degree: 527\n",
      "Book: private peaceful, degree: 364\n",
      "Book: the discoverers: a history of man's search to know his world and himself, degree: 318\n",
      "\n",
      "3. largest community 5: \n",
      "Book: the complete sherlock holmes, degree: 261\n",
      "Book: the fire within, degree: 220\n",
      "Book: the white princess, degree: 214\n",
      "\n",
      "4. largest community 6: \n",
      "Book: the guy not taken: stories, degree: 321\n",
      "Book: and then there were none, degree: 244\n",
      "Book: lipstick jungle, degree: 230\n",
      "\n",
      "5. largest community 3: \n",
      "Book: what we keep, degree: 370\n",
      "Book: suzanne's diary for nicholas, degree: 314\n",
      "Book: family pictures, degree: 306\n",
      "\n",
      "6. largest community 1: \n",
      "Book: the survivors club, degree: 271\n",
      "Book: i, alex cross, degree: 250\n",
      "Book: absolute fear, degree: 248\n",
      "\n",
      "7. largest community 0: \n",
      "Book: zoe's tale, degree: 242\n",
      "Book: one second after, degree: 202\n",
      "Book: shall we tell the president?, degree: 158\n",
      "\n",
      "8. largest community 7: \n",
      "Book: night world, no. 1, degree: 327\n",
      "Book: new moon, degree: 262\n",
      "Book: danse macabre, degree: 213\n",
      "\n",
      "9. largest community 2: \n",
      "Book: just one look, degree: 247\n",
      "Book: the sister, degree: 207\n",
      "Book: in too deep, degree: 200\n",
      "\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "shelf_int_graph = make_all_nodes_ints(shelf_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "shelf_top_3_books = get_top_3_books(shelf_int_graph, shelf_louvain)\n",
    "#print(shelf_top_3_books)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: [(968, 'harry potter and the chamber of secrets'),\n  (941, 'harry potter and the prisoner of azkaban'),\n  (613, 'harry potter and the deathly hallows')]}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shelf_top_3_books"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
