{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import copy\n",
    "import pickle\n",
    "#import community\n",
    "import seaborn as sns\n",
    "import os\n",
    "import ast\n",
    "\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess the two book dataframes\n",
    "- Load dataframes ('https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home', 'https://github.com/malcolmosh/goodbooks-10k-extended/blob/master/README.md')\n",
    "- Remove faulty elements of dataframe without the descriptions\n",
    "- Remove \"additions\" to titles in the description dataframes - Example: title (series #1) -> title\n",
    "- Make a new dataframe that contains \"Book ID\" (from the dataframe without descriptions), \"Book Title\" (-||-), \"genre\" (description dataframe), \"description\" (description dataframe)\n",
    "- Store this dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "descriptions_df = pd.read_csv(DATA_PATH + \"books_descriptions.csv\")\n",
    "book_ID_df = pd.read_json(DATA_PATH + 'goodreads_book_works.json', lines=True)\n",
    "\n",
    "# Remove columns that are not needed in book_ID_df\n",
    "book_ID_df = book_ID_df.filter(items=[\"best_book_id\", \"original_title\", \"reviews_count\"])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List amount of books are in each dataframe\n",
    "print(f\"Amount of books in descriptions_df: {len(descriptions_df)}\")\n",
    "print(f\"Amount of books in book_ID_df: {len(book_ID_df)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove faulty elements from the book_ID_df\n",
    "book_ID_df = book_ID_df[book_ID_df['original_title'] != '']\n",
    "\n",
    "print(f\"Amount of books in book_ID_df: {len(book_ID_df)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove faulty elements from the description dataframe\n",
    "descriptions_df = descriptions_df[descriptions_df['description'].apply(lambda x: isinstance(x, str))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove \"additions\" to titles in the descriptions_df\n",
    "for i, row in tqdm(descriptions_df.iterrows()):\n",
    "    original_title = row[\"title\"]\n",
    "    new_title = re.sub(r'\\((.*)', '', original_title)\n",
    "    descriptions_df.at[i, \"title\"] = new_title.strip()\n",
    "\n",
    "# Check if it worked -> it did\n",
    "# descriptions_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lower case all titles to not have confusion in this manner\n",
    "descriptions_df['title'] = descriptions_df['title'].str.lower()\n",
    "book_ID_df['original_title'] = book_ID_df['original_title'].str.lower()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find corresponding indexes to merge the dataframes\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "titles_not_found = []\n",
    "book_df = pd.DataFrame(columns=['book_id', 'title', 'description', 'genres'])\n",
    "\n",
    "for i, row in tqdm(descriptions_df.iterrows(), total = descriptions_df.shape[0]):\n",
    "    title = row['title']\n",
    "    # Check if the title is in book_ID_df, else append it to the titles_not_found list\n",
    "    if title in book_ID_df['original_title'].values:\n",
    "        # Get all rows that have the a matching title as the current row\n",
    "        temp_df = book_ID_df[book_ID_df['original_title'] == title]\n",
    "\n",
    "        # Get the book id of the book  with the highest amount of reviews\n",
    "        book_id = temp_df['best_book_id'][temp_df['reviews_count'].idxmax()]\n",
    "        descriptions = row['description']\n",
    "        genres = row['genres']\n",
    "        book_df = book_df.append({'book_id': book_id, 'title': title, 'description': descriptions, 'genres': genres}, ignore_index=True)\n",
    "    else:\n",
    "        titles_not_found.append(title)\n",
    "\n",
    "# print the amount of elements that are not found\n",
    "print(f\"Out of the 10000 titles {len(titles_not_found)} are not found in the book_ID_df\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Somehow the same book appears multiple times, hence we drop the duplicates\n",
    "book_df.drop_duplicates(subset=['title'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# TODO - Possibly add the capacity to look through titles and determine if a shorter version could be found in the other dataset (\"harry potter and the sorcersers stone\" becoming \"harry potter and the \" and possibly finding harry potter and the philosophers stone\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess the shelves\n",
    "- Use \"book_id_map.csv\" to find the books we use (ids) and store \"new_ids\" (the ids we can use to find the relevant shelfs)\n",
    "- Drop all rows in \"goodreads_interactions.csv\" that have different ids than \"new_ids\".\n",
    "- Store this dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "book_id_map_df = pd.read_csv(DATA_PATH + \"book_id_map.csv\")\n",
    "book_df = pd.read_csv(DATA_PATH + \"book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create map from book_id to book_id_csv\n",
    "book_id_map = {book_id_map_df['book_id'][i]: book_id_map_df['book_id_csv'][i] for i in range(len(book_id_map_df))}\n",
    "\n",
    "# Change the book ID in our dataset to match the shelf dataset\n",
    "remove_list = [] # remove about 15 books that are for inexplicable reasons not in the shelf dataset\n",
    "for i in range(len(book_df)):\n",
    "    try:\n",
    "        book_df[\"book_id\"][i] = book_id_map[book_df[\"book_id\"][i]]\n",
    "    except:\n",
    "        remove_list.append(i)\n",
    "print(f\"Out of {i} books {len(remove_list)} are not in the shelf dataset and hence removed\")\n",
    "book_df.drop(remove_list, inplace=True)\n",
    "\n",
    "# Save the book_df dataframe with the index change\n",
    "book_df.to_csv(DATA_PATH + 'book_matching_ids_df.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the shelves dataframe (this takes some time and memory)\n",
    "shelves_df = pd.read_csv(DATA_PATH + \"goodreads_interactions.csv\")\n",
    "\n",
    "# Check how many books are on the shelves of ALL users combined\n",
    "print(f'There are {len(shelves_df)} books, copies counted aswell')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove books on the shelves that are not in the book_df\n",
    "shelves_df = shelves_df[shelves_df['book_id'].isin(book_df['book_id'].tolist())]\n",
    "\n",
    "# Save the new shelves_df\n",
    "shelves_df.to_csv(DATA_PATH + 'shelves_df.csv', index=False)\n",
    "\n",
    "# Check how many books are on the shelves of ALL users combined - after removal of books not in book_df\n",
    "print(f\"We have {len(shelves_df)} shelves in total, and in these there are {len(set(shelves_df['book_id'].tolist()))} unique books.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TF-IDF embeddings\n",
    "- Create TF-IDF embeddings\n",
    "- Create them for genres aswell"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports for the text analysis\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Import the book description dataframe\n",
    "book_df = pd.read_csv(DATA_PATH + \"book_matching_ids_df.csv\") # TODO help me not make a \"Unnamed: 0\" column... I want to use the book_id as index, but then it creates this column\n",
    "\n",
    "# Logorithmic scale chosen for IDF\n",
    "BASE = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Function to clean strings (from week 7)\n",
    "def clean_strings(strings):\n",
    "    \"\"\" Cleans a list of strings by removing URLs, numbers, punctuation and stop words\n",
    "\n",
    "    Args:\n",
    "    - strings: a list of strings\n",
    "\n",
    "    returns:\n",
    "    - cleaned_strings: a list of cleaned strings\n",
    "    \"\"\"\n",
    "    cleaned_strings = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for string in strings:\n",
    "        # Remove URLs\n",
    "        string = re.sub(r'http\\S+', '', string)\n",
    "\n",
    "        # Remove numbers\n",
    "        string = re.sub(r'[0-9]', '', string)\n",
    "\n",
    "        # Keep only what is not punctuation\n",
    "        string = re.sub(r'[^\\w\\s]', '', string)\n",
    "\n",
    "        # Lowercase\n",
    "        string = string.lower()\n",
    "\n",
    "        # Remove empty strings and remove stop words and moke them\n",
    "        if len(string) and string not in stop_words and type(string) == str:\n",
    "            cleaned_strings.append(string)\n",
    "\n",
    "    return cleaned_strings\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create tokens from descriptions\n",
    "# If the dataframe has not yet gotten the tokens, do it here and save it\n",
    "if not 'tokens' in book_df.columns:\n",
    "    book_df['tokens'] = None\n",
    "    for i, row in tqdm(book_df.iterrows(), total = book_df.shape[0]):\n",
    "        description = row['description']\n",
    "        # If the description is not a string, it is probably a NaN, so we set it to None\n",
    "        if type(description) == str:\n",
    "            tokens = nltk.word_tokenize(description)\n",
    "            clean_tokens = clean_strings(tokens)\n",
    "            book_df['tokens'][i] = clean_tokens # str(clean_tokens)\n",
    "\n",
    "    book_df.to_csv(DATA_PATH + 'book_matching_ids_df.csv', index=False)\n",
    "else:\n",
    "    book_df['tokens'] = book_df['tokens'].apply(ast.literal_eval)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the tf scores for each community (from the previous weeks)\n",
    "def TF_from_corpus(corpus):\n",
    "    \"\"\" Calculates the TF scores for each word in the corpus\n",
    "\n",
    "    Args:\n",
    "        corpus (list): list of lists of words/strings\n",
    "\n",
    "    Returns:\n",
    "        TF_df (pandas.DataFrame): Dataframe containing the TF scores for each word in the corpus\n",
    "    \"\"\"\n",
    "    # Create empty dictionary to keep track of word counts\n",
    "    word_counts = {}\n",
    "    n_communities = len(corpus)\n",
    "\n",
    "    # Iterate through all communities\n",
    "    for i, document in tqdm(enumerate(corpus), total=n_communities):\n",
    "        # Iterate through each word in the current sublist\n",
    "        for word in document:\n",
    "            # If the current word is not in the dictionary, add it with a list of zeros\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = [0] * n_communities\n",
    "\n",
    "            # Increment count for the current word and list index\n",
    "            word_counts[word][i] += 1\n",
    "\n",
    "    # Create pandas dataframe from the word_counts dictionary\n",
    "    TF_df = pd.DataFrame.from_dict(word_counts).transpose()\n",
    "\n",
    "    return TF_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function that takes in a TF and an IDF and computes the TF_IDF dataframe (from the previous weeks)\n",
    "def make_TF_IDF(TF_df, IDF_dict):\n",
    "    \"\"\"Multiply the TF and IDF scores to get the TF-IDF scores\n",
    "\n",
    "    Args:\n",
    "        TF_df (pandas.DataFrame): Dataframe containing the TF scores for each word in the corpus\n",
    "        IDF_dict (dict): Dictionary containing the IDF scores for each word in the corpus\n",
    "\n",
    "    Returns:\n",
    "        TF_IDF (pandas.DataFrame): Dataframe containing the TF-IDF scores for each word in the corpus\n",
    "    \"\"\"\n",
    "    # Create the TF-IDF dataframe\n",
    "    TF_IDF = pd.DataFrame(index=TF_df.index, columns=TF_df.columns)\n",
    "\n",
    "    # iterate over the index of the DataFrame\n",
    "    for word in tqdm(TF_df.index, total=TF_df.shape[0]):\n",
    "        # multiply the values by the IDF_dict value\n",
    "        TF_IDF.loc[word] = TF_df.loc[word] * IDF_dict[word]\n",
    "\n",
    "\n",
    "    return TF_IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the TF dataframe for the corpus if possible, else create it\n",
    "try:\n",
    "    TF_book_df = pd.read_csv(DATA_PATH + \"TF_book_df.csv\", index_col=0)\n",
    "except:\n",
    "    # Create it\n",
    "    TF_book_df = TF_from_corpus(book_df['tokens'])\n",
    "\n",
    "    # Rename the columns to the book_ids\n",
    "    TF_book_df.columns = book_df['book_id'].tolist()\n",
    "\n",
    "    # Save the TF dataframe\n",
    "    TF_book_df.to_csv(DATA_PATH + \"TF_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the total token count \"T_all_books\" and the IDF score for each book \"IDF_book_dict\"\n",
    "try:\n",
    "    IDF_dict = np.load(DATA_PATH + 'IDF_dict.npy', allow_pickle=True).item()\n",
    "except:\n",
    "    T_all_books = TF_book_df.apply(lambda row: (row != 0).sum(), axis=1)\n",
    "    # The log BASE is chosen when loading the libraries\n",
    "    IDF_dict = {word: np.emath.logn(BASE, len(TF_book_df.columns)/ T_all_books[word]) for word in TF_book_df.index}\n",
    "\n",
    "    np.save(DATA_PATH + 'IDF_dict.npy', IDF_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the TF-IDF scores for each book \"TF_IDF_book_df\" if it has not already been made\n",
    "try:\n",
    "    TF_IDF_book_df = pd.read_csv(DATA_PATH + \"TF_IDF_book_df.csv\", index_col=0)\n",
    "except:\n",
    "    # Create the dataframe\n",
    "    TF_IDF_book_df = make_TF_IDF(TF_book_df, IDF_dict)\n",
    "\n",
    "    # Save the dataframe\n",
    "    TF_IDF_book_df.to_csv(DATA_PATH + \"TF_IDF_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Genre TF and TF_IDF scores\n",
    "- Do this by having all books with a genre define the \"document\" for that genre\n",
    "- Then compute the \"TF_genre_df\" dataframe, by summing all books from \"TF_book_df\" from that genre\n",
    "- Here we make the decision that the IDF is the same as for the books.\n",
    "    - (Alternatively one could have weighed each book and made a new IDF score, however, this weighs a book with twice as many genres twice as large, hence we use the other option)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the set of genres\n",
    "genres = set()\n",
    "for i in book_df[\"genres\"].to_list():\n",
    "    genres = genres.union(set(ast.literal_eval(i)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try to load the TF_genres_df, else create it\n",
    "try:\n",
    "    # Load the TF_genres_df\n",
    "    TF_genres_df = pd.read_csv(DATA_PATH + \"TF_genres_df.csv\", index_col=0)\n",
    "except:\n",
    "    # For each genre, sum all TF scores for books in that genre\n",
    "    TF_genres_df = pd.DataFrame(index=TF_book_df.index, columns=genres)\n",
    "\n",
    "    TF_genres_df = TF_genres_df.fillna(0)\n",
    "\n",
    "    # Go through all books and add the TF scores to the genres of the book\n",
    "    for i, row in tqdm(book_df.iterrows()):\n",
    "        for genre in ast.literal_eval(row['genres']):\n",
    "            TF_genres_df[genre] = TF_genres_df[genre] + TF_book_df[row['book_id']]\n",
    "\n",
    "    # Save the genres_TF_df\n",
    "    TF_genres_df.to_csv(DATA_PATH + \"TF_genres_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try to load the TF_IDF_genres_df, else create it\n",
    "try:\n",
    "    # Load the TF_IDF_genres_df\n",
    "    TF_IDF_genres_df = pd.read_csv(DATA_PATH + \"TF_IDF_genres_df.csv\", index_col=0)\n",
    "except:\n",
    "    # Create the dataframe\n",
    "    TF_IDF_genres_df = make_TF_IDF(TF_genres_df, IDF_dict)\n",
    "\n",
    "    # Save the dataframe\n",
    "    TF_IDF_genres_df.to_csv(DATA_PATH + \"TF_IDF_genres_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Have not added things with # delete"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a genre for each book\n",
    "- make the inner product which each book and the genre vector (both normed)\n",
    "- let the largest inner product that the book contains be the genre of the book"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Inner product function\n",
    "def inner_product(v1, v2):\n",
    "    \"\"\"Calculates the normed inner product of two vectors\n",
    "\n",
    "    Args:\n",
    "        v1 (list): list of numbers\n",
    "        v2 (list): list of numbers\n",
    "\n",
    "    Returns:\n",
    "        inner_product (float): inner product of the two vectors (divided by the product of their norms)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Generate genre by taking inner products between each book and each genre of that book, and choosing the maximal\n",
    "def get_genres(book_df, TF_IDF_genres_df, TF_IDF_book_df):\n",
    "    # TODO: Check these args and return\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        book_df (DataFrame): DataFrame with genres for each book\n",
    "        TF_IDF_genres_df (DataFrame): DataFrame with TF_IDF each genre\n",
    "        TF_IDF_book_df (DataFrame): DataFrame with TF_IDF each book\n",
    "\n",
    "    Returns:\n",
    "        genres (dict): Dictionary with book ids as keys and the corresponding gerne that matches the best\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary to store the genre for each book\n",
    "    genres = {}\n",
    "\n",
    "    # Go through each book\n",
    "    for i, row in tqdm(book_df.iterrows()):\n",
    "        # Get the TF-IDF scores for the current book\n",
    "        book_TF_IDF = TF_IDF_book_df[row[\"book_id\"]]\n",
    "\n",
    "        best_genre = (None, 0)\n",
    "\n",
    "        # Go through each genre of the book\n",
    "        for genre in ast.literal_eval(row['genres']):\n",
    "            genre_TF_IDF = TF_IDF_genres_df[genre]\n",
    "            # print(f\"inner_product {inner_product(book_TF_IDF, genre_TF_IDF)} {genre}\") # Testing\n",
    "            if inner_product(book_TF_IDF, genre_TF_IDF) > best_genre[1]:\n",
    "                best_genre = (genre, inner_product(book_TF_IDF, genre_TF_IDF))\n",
    "        # print(best_genre) # Testing\n",
    "        # break # Testing\n",
    "        # Save the best genre\n",
    "        genres[row[\"book_id\"]] = best_genre[0]\n",
    "\n",
    "    return genres"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the genres for each book\n",
    "genres = get_genres(book_df, TF_IDF_genres_df, TF_IDF_book_df)\n",
    "\n",
    "book_df[\"top_genre\"] = book_df[\"book_id\"].map(genres)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the book_df as complete_book_df\n",
    "book_df.to_csv(DATA_PATH + \"complete_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis\n",
    "Choosing a threshold for the graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the shelf edges and store them (this takes about 12 hours)\n",
    "shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')\n",
    "# Collapse into shelves\n",
    "imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "\n",
    "# Count occurrences of each book\n",
    "book_counts = {}\n",
    "for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "    for book in shelf['book_id']:\n",
    "        if book in book_counts:\n",
    "            book_counts[book] += 1\n",
    "        else:\n",
    "            book_counts[book] = 1\n",
    "\n",
    "# Count appearances of pairs of books\n",
    "edges = {}\n",
    "for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "    if len(shelf['book_id']) > 1:\n",
    "        # Generate pairs of books\n",
    "        for i in range(len(shelf['book_id'])):\n",
    "            for j in range(i+1, len(shelf['book_id'])):\n",
    "                # Create edge\n",
    "                edge = frozenset([shelf['book_id'][i], shelf['book_id'][j]])\n",
    "                # Add edge to dictionary\n",
    "                if edge in edges:\n",
    "                    edges[edge] += 1\n",
    "                else:\n",
    "                    edges[edge] = 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# load data\n",
    "if 'shelves_df' not in locals():\n",
    "    shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of users are: 818569\n"
     ]
    }
   ],
   "source": [
    "# Implode dataframe to get books for each user in a list for each\n",
    "imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "print(f'The amount of users are: {len(imploded_df)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the full network there are 28704535 edges\n"
     ]
    }
   ],
   "source": [
    "#Get all the edges:\n",
    "edges_orginal = np.load(DATA_PATH + \"shelf_edges.npy\", allow_pickle=True).item()\n",
    "print(f'In the full network there are {len(edges_orginal)} edges')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot a histogram of the number of times each edge appears\n",
    "plt.hist(edges_orginal.values(), log=True, bins=100)\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('People who had booth books')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of books appearing together')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: That amount of edges is way to much comment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### See what thresholds would be better"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "edges_weighted = edges_orginal.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO comment on the method for weighting the edges"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 818569/818569 [00:43<00:00, 18751.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each book\n",
    "book_counts = {}\n",
    "for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "    for book in shelf['book_id']:\n",
    "        if book in book_counts:\n",
    "            book_counts[book] += 1\n",
    "        else:\n",
    "            book_counts[book] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28704535/28704535 [00:27<00:00, 1050938.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#For each edge divide the weight of it by the lowest appearence count of the two nodes it connects\n",
    "for edge in tqdm(edges_weighted):\n",
    "            edges_weighted[edge] = edges_weighted[edge] / min((book_counts[list(edge)[0]], book_counts[list(edge)[1]]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Save weighted edges\n",
    "np.save(DATA_PATH + 'shelf_edges_weighted.npy', edges_weighted)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot a histogram of the number of times each edge appears\n",
    "plt.hist(edges_weighted.values(), log=True, bins=100)\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('People who had booth books')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of books appearing together')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "while threshold <= 0.8:\n",
    "    edges_threshold = {k: v for k, v in edges_weighted.items() if v >= threshold}\n",
    "\n",
    "    nodes = {node for nodes in edges_threshold for node in nodes}\n",
    "\n",
    "    print(f'For the threshold of {np.round(threshold,2)}:')\n",
    "    print(f'The amount of edges is {len(edges_threshold)}, nodes: {len(nodes)} and the node/edges fraction is: {len(edges_threshold)*2/len(nodes)}')\n",
    "    print()\n",
    "    threshold += 0.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Todo: Snak om overvejelserne her\n",
    "# tager 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create network\n",
    "- Load dataframe with shelves\n",
    "- Implode dataframe to make shelves into edgelist\n",
    "- Calculate assortativity\n",
    "    - Genre\n",
    "    - Degrees\n",
    "- Get largest sub network\n",
    "- Create communities\n",
    "    - Color according to genre"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Create the network\n",
    "#TODO: This an old one check if the next isn't better\n",
    "def create_network(threshold=0.5):\n",
    "    \"\"\"\n",
    "    Creates the network from the shelf edges.\n",
    "    If the edges have already been created, they are loaded.\n",
    "    Otherwise, they are created and stored. The edges are filtered by the threshhold. The network is returned.\n",
    "\n",
    "    Args:\n",
    "        threshhold (int): The minimum number of times an edge must appear to be included in the network.\n",
    "\n",
    "    Returns:\n",
    "        G (networkx.Graph): The network of book edges.\n",
    "    \"\"\"\n",
    "    # If the edges have already been created, load them\n",
    "    try:\n",
    "        # Load the shelf edges\n",
    "        edges = np.load(DATA_PATH + 'shelf_edges_weighted.npy', allow_pickle=True).item() # (takes about 10 minutes)\n",
    "    except:\n",
    "        # Create the shelf edges and store them (this takes about 12 hours)\n",
    "        shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')\n",
    "        # Collapse into shelves\n",
    "        imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "\n",
    "        # Count occurrences of each book\n",
    "        book_counts = {}\n",
    "        for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "            for book in shelf['book_id']:\n",
    "                if book in book_counts:\n",
    "                    book_counts[book] += 1\n",
    "                else:\n",
    "                    book_counts[book] = 1\n",
    "\n",
    "        # Count appearances of pairs of books\n",
    "        edges = {}\n",
    "        for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "            if len(shelf['book_id']) > 1:\n",
    "                # Generate pairs of books\n",
    "                for i in range(len(shelf['book_id'])):\n",
    "                    for j in range(i+1, len(shelf['book_id'])):\n",
    "                        # Create edge\n",
    "                        edge = frozenset([shelf['book_id'][i], shelf['book_id'][j]])\n",
    "                        # Add edge to dictionary\n",
    "                        if edge in edges:\n",
    "                            edges[edge] += 1\n",
    "                        else:\n",
    "                            edges[edge] = 1\n",
    "\n",
    "        # Weigh edges by dividing the count by the lowest amount of appearances between the two books\n",
    "        for edge in edges:\n",
    "            edges[edge] = edges[edge] / min(book_counts[list(edge)[0]], book_counts[list(edge)[1]])\n",
    "\n",
    "        # Store the edges\n",
    "        np.save(DATA_PATH + 'shelf_edges.npy', edges)\n",
    "\n",
    "    # Remove edges that are below the threshhold\n",
    "    edges = {k: v for k, v in tqdm(edges.items()) if v >= threshold}\n",
    "\n",
    "    # Create the network\n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(([(u, v, w) for (u, v), w in edges.items()]))\n",
    "\n",
    "    return G\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def create_network(threshold=0.2):\n",
    "\n",
    "    Creates the network from the shelf edges.\n",
    "    If the edges have already been created, they are loaded.\n",
    "    Otherwise, they are created and stored. The edges are filtered by the threshold. The network is returned.\n",
    "\n",
    "    Args:\n",
    "        threshold (int): The minimum number of times an edge must appear to be included in the network.\n",
    "\n",
    "    Returns:\n",
    "        G (networkx.Graph): The network of book edges.\n",
    "\n",
    "    # If the edges have already been created, load them\n",
    "    try:\n",
    "        # Load the shelf edges\n",
    "        edges = np.load(DATA_PATH + 'shelf_edges.npy', allow_pickle=True).item() # (takes about 10 minutes)\n",
    "    except:\n",
    "        # Create the shelf edges and store them (this takes about 12 hours)\n",
    "        shelves_df = pd.read_csv(DATA_PATH + 'shelves_df.csv')\n",
    "        # Collapse into shelves\n",
    "        imploded_df = shelves_df.groupby('user_id')['book_id'].apply(list).reset_index()\n",
    "\n",
    "        # Count occurrences of each book\n",
    "        book_counts = {}\n",
    "        for i, shelf in tqdm(imploded_df.iterrows(), total=imploded_df.shape[0]):\n",
    "            for book in shelf['book_id']:\n",
    "                if book in book_counts:\n",
    "                    book_counts[book] += 1\n",
    "                else:\n",
    "                    book_counts[book] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # Load the edges\n",
    "    edges = np.load(DATA_PATH + 'shelf_edges.npy', allow_pickle=True)\n",
    "\n",
    "    # Remove edges that are below the threshold\n",
    "    edges = {k: v for k, v in edges.items() if v >= threshold}\n",
    "\n",
    "    # Create the network\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges.keys())\n",
    "\n",
    "    return G\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "if not \"complete_book_df\" in locals():\n",
    "    complete_book_df = pd.read_csv(DATA_PATH + \"complete_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Give nodes attributes\n",
    "def make_attributes(df, graph):\n",
    "    #TODO: args and returns\n",
    "    # make a dictionary with attributes for each node\n",
    "    book_attributes = dict()\n",
    "    for i, book in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        node = book['book_id']\n",
    "        if not node in graph.nodes():\n",
    "            continue\n",
    "        book_attributes[node] = dict()\n",
    "        top_genre = book['top_genre']\n",
    "        title = book['title']\n",
    "        genres = eval(book['genres'])\n",
    "\n",
    "        book_attributes[node]['title'] = title\n",
    "        book_attributes[node]['genres'] = genres\n",
    "        book_attributes[node]['top_genre'] = top_genre\n",
    "    return book_attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28704535/28704535 [00:05<00:00, 4957287.97it/s]\n",
      "100%|██████████| 7676/7676 [00:00<00:00, 18301.09it/s]\n"
     ]
    },
    {
     "ename": "NetworkXError",
     "evalue": "GraphML writer does not support <class 'list'> as data values.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:164\u001B[0m, in \u001B[0;36mwrite_graphml_lxml\u001B[1;34m(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 164\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlxml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01metree\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mlxmletree\u001B[39;00m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'lxml'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNetworkXError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m nx\u001B[38;5;241m.\u001B[39mset_node_attributes(shelf_graph, shelf_attribute_dict)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Save the network\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[43mnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_graphml\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshelf_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDATA_PATH\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshelves_network_05.graphml\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<class 'networkx.utils.decorators.argmap'> compilation 5:5\u001B[0m, in \u001B[0;36margmap_write_graphml_lxml_1\u001B[1;34m(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgzip\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m defaultdict\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:166\u001B[0m, in \u001B[0;36mwrite_graphml_lxml\u001B[1;34m(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)\u001B[0m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlxml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01metree\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mlxmletree\u001B[39;00m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m--> 166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrite_graphml_xml\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mG\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprettyprint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_numeric_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnamed_key_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43medge_id_from_attribute\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    176\u001B[0m writer \u001B[38;5;241m=\u001B[39m GraphMLWriterLxml(\n\u001B[0;32m    177\u001B[0m     path,\n\u001B[0;32m    178\u001B[0m     graph\u001B[38;5;241m=\u001B[39mG,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    183\u001B[0m     edge_id_from_attribute\u001B[38;5;241m=\u001B[39medge_id_from_attribute,\n\u001B[0;32m    184\u001B[0m )\n\u001B[0;32m    185\u001B[0m writer\u001B[38;5;241m.\u001B[39mdump()\n",
      "File \u001B[1;32m<class 'networkx.utils.decorators.argmap'> compilation 10:5\u001B[0m, in \u001B[0;36margmap_write_graphml_xml_6\u001B[1;34m(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgzip\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m defaultdict\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:112\u001B[0m, in \u001B[0;36mwrite_graphml_xml\u001B[1;34m(G, path, encoding, prettyprint, infer_numeric_types, named_key_ids, edge_id_from_attribute)\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Write G in GraphML XML format to path\u001B[39;00m\n\u001B[0;32m     72\u001B[0m \n\u001B[0;32m     73\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;124;03mand unidirected edges together) hyperedges, nested graphs, or ports.\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    105\u001B[0m writer \u001B[38;5;241m=\u001B[39m GraphMLWriter(\n\u001B[0;32m    106\u001B[0m     encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[0;32m    107\u001B[0m     prettyprint\u001B[38;5;241m=\u001B[39mprettyprint,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    110\u001B[0m     edge_id_from_attribute\u001B[38;5;241m=\u001B[39medge_id_from_attribute,\n\u001B[0;32m    111\u001B[0m )\n\u001B[1;32m--> 112\u001B[0m \u001B[43mwriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_graph_element\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    113\u001B[0m writer\u001B[38;5;241m.\u001B[39mdump(path)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:649\u001B[0m, in \u001B[0;36mGraphMLWriter.add_graph_element\u001B[1;34m(self, G)\u001B[0m\n\u001B[0;32m    646\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (xml_obj, data) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattributes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    647\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (k, v, scope, default) \u001B[38;5;129;01min\u001B[39;00m data:\n\u001B[0;32m    648\u001B[0m         xml_obj\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 649\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattr_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscope\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscope\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault\u001B[49m\n\u001B[0;32m    651\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    652\u001B[0m         )\n\u001B[0;32m    653\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mxml\u001B[38;5;241m.\u001B[39mappend(graph_element)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\readwrite\\graphml.py:561\u001B[0m, in \u001B[0;36mGraphMLWriter.add_data\u001B[1;34m(self, name, element_type, value, scope, default)\u001B[0m\n\u001B[0;32m    556\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    557\u001B[0m \u001B[38;5;124;03mMake a data element for an edge or a node. Keep a log of the\u001B[39;00m\n\u001B[0;32m    558\u001B[0m \u001B[38;5;124;03mtype in the keys table.\u001B[39;00m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    560\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m element_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mxml_type:\n\u001B[1;32m--> 561\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m nx\u001B[38;5;241m.\u001B[39mNetworkXError(\n\u001B[0;32m    562\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGraphML writer does not support \u001B[39m\u001B[38;5;132;01m{\u001B[39;00melement_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m as data values.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    563\u001B[0m     )\n\u001B[0;32m    564\u001B[0m keyid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_key(name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_xml_type(element_type), scope, default)\n\u001B[0;32m    565\u001B[0m data_element \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmyElement(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m, key\u001B[38;5;241m=\u001B[39mkeyid)\n",
      "\u001B[1;31mNetworkXError\u001B[0m: GraphML writer does not support <class 'list'> as data values."
     ]
    }
   ],
   "source": [
    "folder_path = DATA_PATH\n",
    "file_name = \"shelves_network_05.graphml\" #TODO: change to 05\n",
    "\n",
    "if not os.path.exists(os.path.join(folder_path, file_name)):\n",
    "    # Make the network\n",
    "    shelf_graph = create_network(0.5)\n",
    "\n",
    "    #Give nodes attributes\n",
    "    shelf_attribute_dict = make_attributes(complete_book_df, shelf_graph)\n",
    "    nx.set_node_attributes(shelf_graph, shelf_attribute_dict)\n",
    "\n",
    "    # Save the network\n",
    "    with open(DATA_PATH + 'shelves_network_05.pickle', 'wb') as f:\n",
    "        pickle.dump(shelf_graph, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 7539 nodes and 109627 edges\n"
     ]
    }
   ],
   "source": [
    "print(test_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create NLP network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Used to make all node ids ints\n",
    "def make_all_nodes_ints(graph):\n",
    "    #TODO: args and returns\n",
    "\n",
    "    # Check if first id is a string, in which case, don't do anyting\n",
    "    if isinstance(list(graph.nodes())[0], int):\n",
    "            return graph\n",
    "\n",
    "    # Make a mapping so each node can be renamed into what it was, but a string instead\n",
    "    node_map = {}\n",
    "    for node in graph.nodes():\n",
    "        node_map[node] = int(node)\n",
    "    str_graph = nx.relabel_nodes(graph, node_map)\n",
    "\n",
    "    return str_graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "if 'shelf_graph' not in locals():\n",
    "    shelf_graph = nx.read_graphml(DATA_PATH + 'shelves_graph_04.graphml')#TODO ændre til 05 efter den er lavet\n",
    "    shelf_graph = make_all_nodes_ints(shelf_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: Comment on how we make the NLP network (Inner products)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the TF_IDF values for each book\n",
    "tf_idf_book = pd.read_csv(DATA_PATH + \"TF_IDF_book_df.csv\", index_col=0)\n",
    "book_df = pd.read_csv(DATA_PATH + \"complete_book_df.csv\", index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inner product function\n",
    "def inner_product(v1, v2):\n",
    "    \"\"\"Calculates the normed inner product of two vectors\n",
    "\n",
    "    Args:\n",
    "        v1 (list): list of numbers\n",
    "        v2 (list): list of numbers\n",
    "\n",
    "    Returns:\n",
    "        inner_product (float): inner product of the two vectors (divided by the product of their norms)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges were loaded\n"
     ]
    }
   ],
   "source": [
    "# Create the inner product between every book\n",
    "try:\n",
    "    NLP_edges = np.load(DATA_PATH + 'NLP_edges.npy', allow_pickle=True).item()\n",
    "    print('Edges were loaded')\n",
    "except:\n",
    "    similarities = {}\n",
    "    for i in tqdm(range(len(book_df))):\n",
    "        for j in range(i+1, len(book_df)):\n",
    "            similarities[(book_df.iloc[i][\"book_id\"], book_df.iloc[j][\"book_id\"])] = inner_product(tf_idf_book[str(book_df.iloc[i][\"book_id\"])], tf_idf_book[str(book_df.iloc[j][\"book_id\"])])\n",
    "\n",
    "    # Save the edges\n",
    "    np.save(DATA_PATH + 'NLP_edges.npy', similarities)\n",
    "    NLP_edges = similarities\n",
    "    print('Edges were saved')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The threshold to have the same number of edges in the NLP as the shelf graph is: 0.043\n"
     ]
    }
   ],
   "source": [
    "# Find the value that separates the top 256679 larges edges from the rest\n",
    "threshold = sorted(NLP_edges.values(), reverse=True)[shelf_graph.number_of_edges()]\n",
    "print(f'The threshold to have the same number of edges in the NLP as the shelf graph is: {np.round(threshold,3)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "NLP_edges = {k: v for k, v in NLP_edges.items() if v >= threshold}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 7671 nodes and 256680 edges\n"
     ]
    }
   ],
   "source": [
    "NLP_graph = nx.Graph()\n",
    "NLP_graph.add_weighted_edges_from(([(u, v, w) for (u, v), w in NLP_edges.items()]))\n",
    "print(NLP_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Give nodes attributes\n",
    "NLP_attribute_dict = make_attributes(complete_book_df, NLP_graph)\n",
    "nx.set_node_attributes(NLP_graph, NLP_attribute_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save NLP_graph\n",
    "nx.write_graphml(NLP_graph, DATA_PATH + 'Nlp_graph_05.graphml') #TODo: run with new 05 shelf graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Matching attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "if 'shelf_graph' not in locals():\n",
    "    shelf_graph = nx.read_graphml(DATA_PATH + 'shelves_graph_04.graphml') #TODO ændres til 05\n",
    "    shelf_graph = make_all_nodes_ints(shelf_graph)\n",
    "\n",
    "if 'NLP_graph' not in locals():\n",
    "    NLP_graph = nx.read_graphml(DATA_PATH + 'NLP_graph_04.graphml') #TODO ændres til 05\n",
    "    NLP_graph = make_all_nodes_ints(NLP_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# Function to calculate the fraction of neighbors that have the same attribute value\n",
    "def get_matching_att(graph, att):\n",
    "    \"\"\"\n",
    "    A function that calculates the fraction of neighbors that have the same attribute value\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The graph to be analyzed\n",
    "        att (str): The attribute to be analyzed\n",
    "\n",
    "    Returns:\n",
    "        match_frac (dict): A dictionary with the fraction of neighbors that have the same attribute value\n",
    "    \"\"\"\n",
    "    match_frac = {}\n",
    "    for node in tqdm(graph.nodes()):\n",
    "        counter = 0\n",
    "        neighbors = len(list(graph.neighbors(node)))\n",
    "\n",
    "        if neighbors == 0: match_frac[node] = 0 # Have to check this, otherwise division by zero\n",
    "\n",
    "        else:\n",
    "            for neighbor in graph.neighbors(node):\n",
    "                # Check if the attribute value is the same for the node and the neighbor\n",
    "                if nx.get_node_attributes(graph, att)[neighbor] == nx.get_node_attributes(graph, att)[node]:\n",
    "                    counter += 1\n",
    "\n",
    "            match_frac[node] = counter / neighbors\n",
    "\n",
    "    return match_frac"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(shelves) Average across all nodes 0.22811813442165993\n",
      "(NLP) Average across all nodes 0.1997208605593006\n"
     ]
    }
   ],
   "source": [
    "# Get the average fraction of neighbors that have the same attribute value\n",
    "top_field_fracs_shelves = get_matching_att(shelf_graph, 'top_genre')\n",
    "top_field_fracs_NLP = get_matching_att(NLP_graph, 'top_genre')\n",
    "\n",
    "print(f\"(shelves) Average across all nodes {np.mean(list(top_field_fracs_shelves.values()))}\")\n",
    "print(f\"(NLP) Average across all nodes {np.mean(list(top_field_fracs_NLP.values()))}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Create a new graph, with the same nodes and edges, but where the association between nodes and field is shuffled. Compute the measure above for this randomized graph."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# Function that shuffles the attribute values of a graph\n",
    "def shuffle_node_att(graph, att):\n",
    "    \"\"\"\n",
    "    A function that shuffles the attribute values of a graph\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The graph to be analyzed\n",
    "        att (str): The attribute to be analyzed\n",
    "\n",
    "    Returns:\n",
    "        graph (nx.Graph): The graph with shuffled attributes\n",
    "    \"\"\"\n",
    "    # Make dictionary with shuffled attributes\n",
    "    shuffled_atts = nx.get_node_attributes(graph, att)\n",
    "    temp = list(shuffled_atts.values())\n",
    "    random.shuffle(temp)\n",
    "    new_atts = dict(zip(shuffled_atts, temp))\n",
    "\n",
    "    # Set the shuffled attributes\n",
    "    nx.set_node_attributes(graph, new_atts, att)\n",
    "\n",
    "    return graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Shelves) When the fields are randomly assigned, on average an genre has 0.22811813442165993 of their neighbors in the same genre\n",
      "(NLP) When the fields are randomly assigned, on average an genre has 0.04969976529917715 of their neighbors in the same genre\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the graph and shuffle the attribute values\n",
    "shelf_giant_copy = copy.deepcopy(shelf_graph)\n",
    "shelf_giant_shuffled = shuffle_node_att(shelf_graph, 'top_genre')\n",
    "\n",
    "NLP_giant_copy = copy.deepcopy(NLP_graph)\n",
    "NLP_giant_shuffled = shuffle_node_att(NLP_graph, 'top_genre')\n",
    "\n",
    "print(f\"(Shelves) When the fields are randomly assigned, on average an genre has {np.mean(list(get_matching_att(shelf_giant_copy, 'top_genre').values()))} of their neighbors in the same genre\")\n",
    "print(f\"(NLP) When the fields are randomly assigned, on average an genre has {np.mean(list(get_matching_att(NLP_giant_shuffled, 'top_genre').values()))} of their neighbors in the same genre\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import netwulf\n",
    "import ast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "shelf_graph = nx.read_graphml(DATA_PATH + 'shelves_graph_04.graphml')\n",
    "shelf_graph = make_all_nodes_ints(shelf_graph)\n",
    "\n",
    "NLP_graph = nx.read_graphml(DATA_PATH + 'NLP_graph_04.graphml')\n",
    "NLP_graph = make_all_nodes_ints(NLP_graph)\n",
    "\n",
    "#Load communities\n",
    "shelf_louvain = np.load(DATA_PATH + 'shelves_communities_04.npy', allow_pickle = True)\n",
    "NLP_louvain = np.load(DATA_PATH + 'NLP_communities_04.npy', allow_pickle= True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "if not \"complete_book_df\" in locals():\n",
    "    complete_book_df = pd.read_csv(DATA_PATH + \"complete_book_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def make_attributes(df, graph):\n",
    "    #TODO: args and returns\n",
    "    # make a dictionary with attributes for each node\n",
    "    book_attributes = dict()\n",
    "    for i, book in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        node = book['book_id']\n",
    "        if not node in graph.nodes():\n",
    "            continue\n",
    "        book_attributes[node] = dict()\n",
    "        top_genre = book['top_genre']\n",
    "        title = book['title']\n",
    "        genres = ast.literal_eval(book['genres'])\n",
    "\n",
    "        book_attributes[node]['title'] = title\n",
    "        book_attributes[node]['genres'] = genres\n",
    "        book_attributes[node]['top_genre'] = top_genre\n",
    "    return book_attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7676/7676 [00:00<00:00, 21558.65it/s]\n",
      "100%|██████████| 7676/7676 [00:00<00:00, 23929.70it/s]\n"
     ]
    }
   ],
   "source": [
    "shelf_attribute_dict = make_attributes(complete_book_df, shelf_graph)\n",
    "NLP_attribute_dict = make_attributes(complete_book_df, NLP_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "nx.set_node_attributes(shelf_graph, shelf_attribute_dict)\n",
    "nx.set_node_attributes(NLP_graph, NLP_attribute_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "# List of colors to use for the visualization\n",
    "\n",
    "\n",
    "# Use netwulf to visualize the communities\n",
    "def visualize_communities(graph, communities):\n",
    "    colors = list(mcolors.CSS4_COLORS.values())[20:len(communities) + 20]\n",
    "    # Create a dictionary that maps nodes to communities\n",
    "    node_to_community = {}\n",
    "    for i, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            node = str(node)\n",
    "            node_to_community[node] = i\n",
    "\n",
    "    # Add a color attribute to the nodes\n",
    "    for node in graph.nodes:\n",
    "        # Add the community number as a node attribute\n",
    "        graph.nodes[node][\"color\"] = colors[node_to_community[node]]\n",
    "\n",
    "    # Visualize the graph with netwulf\n",
    "    netwulf.visualize(graph, config={\"Node color\": \"color\",\n",
    "                                    'node_size': 50,\n",
    "                                    'link_width': 0.1,\n",
    "                                    'link_alpha': 0.01,\n",
    "                                    'node_size_variation': 0.3,\n",
    "                                    'zoom': 0.65,\n",
    "                                    'scale_node_size_by_strength': True,})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "visualize_communities(shelf_graph, shelf_louvain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "visualize_communities(NLP_graph, NLP_louvain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Colors for genre"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def create_genre_dict(graph):\n",
    "    genre_dict = {}\n",
    "    for node in tqdm(graph.nodes):\n",
    "        genre = nx.get_node_attributes(graph, 'top_genre')[node]\n",
    "        try:\n",
    "            genre_dict[genre].append(node)\n",
    "        except:\n",
    "            genre_dict[genre] = [node]\n",
    "    return genre_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7665/7665 [00:12<00:00, 612.85it/s]\n",
      "100%|██████████| 7671/7671 [00:12<00:00, 611.70it/s]\n"
     ]
    }
   ],
   "source": [
    "shelf_genre_dict = create_genre_dict(shelf_graph)\n",
    "NLP_genre_dict = create_genre_dict(NLP_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Use netwulf to visualize the genres\n",
    "def visualize_genres(graph, genre_dict):\n",
    "    colors = list(mcolors.CSS4_COLORS.values())[20:len(genre_dict.keys())+20]\n",
    "    # Create a dictionary that maps nodes to genre\n",
    "    node_to_genre = {}\n",
    "    for i, genre in enumerate(genre_dict.keys()):\n",
    "        for node in genre_dict[genre]:\n",
    "            node_to_genre[node] = i\n",
    "\n",
    "\n",
    "\n",
    "    # Add a color attribute to the nodes\n",
    "    for node in graph.nodes:\n",
    "        # Add the community number as a node attribute\n",
    "        graph.nodes[node][\"color\"] = colors[node_to_genre[node]]\n",
    "\n",
    "    # Visualize the graph with netwulf\n",
    "    netwulf.visualize(graph, config={\"Node color\": \"color\",\n",
    "                                    'node_size': 50,\n",
    "                                    'link_width': 0.1,\n",
    "                                    'link_alpha': 0.01,\n",
    "                                    'node_size_variation': 0.3,\n",
    "                                    'zoom': 0.65,\n",
    "                                    'scale_node_size_by_strength': True,})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "visualize_genres(shelf_graph, shelf_genre_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "visualize_genres(NLP_graph, NLP_genre_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate wordclouds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Find top three books for each community according to degree\n",
    "def get_top_3_books(graph_type, louvain_groups):\n",
    "    top_3_books = {}\n",
    "    # Store the top 3 books by degree for all communities\n",
    "    for i, community in enumerate(louvain_groups):\n",
    "\n",
    "        # Get the top 3 books by degree\n",
    "        sorted_dict = sorted(dict(graph_type.degree(community)).items(), key=lambda x: x[1], reverse=True)\n",
    "        names = nx.get_node_attributes(graph_type, \"title\")\n",
    "        top_3_keys = [k for k, v in sorted_dict[:3]] # Get the ID\n",
    "        top_3_names = [names[k] for k in top_3_keys] # get the name\n",
    "\n",
    "        top_3_books[i] = [(top_3_keys[j], top_3_names[j]) for j in range(len(top_3_keys))]\n",
    "        return top_3_books"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shelf graph\n",
      "**********\n",
      "1. largest community 1: \n",
      "Book: the hunger games, degree: 7312\n",
      "Book: to kill a mockingbird, degree: 6527\n",
      "Book: catching fire, degree: 5683\n",
      "\n",
      "2. largest community 0: \n",
      "Book: harry potter and the chamber of secrets, degree: 5694\n",
      "Book: harry potter and the prisoner of azkaban, degree: 5530\n",
      "Book: harry potter and the deathly hallows, degree: 5394\n",
      "\n",
      "3. largest community 5: \n",
      "Book: eclipse, degree: 2309\n",
      "Book: breaking dawn, degree: 2232\n",
      "Book: city of bones, degree: 1947\n",
      "\n",
      "4. largest community 3: \n",
      "Book: angels & demons, degree: 1024\n",
      "Book: a time to kill, degree: 151\n",
      "Book: along came a spider, degree: 145\n",
      "\n",
      "5. largest community 2: \n",
      "Book: the riddle, degree: 1384\n",
      "Book: a wrinkle in time, degree: 771\n",
      "Book: charlotte's web, degree: 657\n",
      "\n",
      "6. largest community 9: \n",
      "Book: fifty shades of grey, degree: 890\n",
      "Book: beautiful disaster, degree: 496\n",
      "Book: fifty shades darker, degree: 399\n",
      "\n",
      "7. largest community 6: \n",
      "Book: dark lover, degree: 498\n",
      "Book: dead until dark, degree: 405\n",
      "Book: halfway to the grave, degree: 291\n",
      "\n",
      "8. largest community 4: \n",
      "Book: the shining, degree: 322\n",
      "Book: the stand, degree: 259\n",
      "Book: dark visions, degree: 258\n",
      "\n",
      "9. largest community 7: \n",
      "Book: the tipping point: how little things can make a big difference, degree: 101\n",
      "Book: outliers: the story of success, degree: 96\n",
      "Book: freakonomics: a rogue economist explores the hidden side of everything, degree: 72\n",
      "\n",
      "NLP graph\n",
      "**********\n",
      "1. largest community 4: \n",
      "Book: abandon, degree: 667\n",
      "Book: fueled, degree: 535\n",
      "Book: wait for it, degree: 454\n",
      "\n",
      "2. largest community 8: \n",
      "Book: the greatest generation, degree: 527\n",
      "Book: private peaceful, degree: 364\n",
      "Book: the discoverers: a history of man's search to know his world and himself, degree: 318\n",
      "\n",
      "3. largest community 5: \n",
      "Book: the complete sherlock holmes, degree: 261\n",
      "Book: the fire within, degree: 220\n",
      "Book: the white princess, degree: 214\n",
      "\n",
      "4. largest community 6: \n",
      "Book: the guy not taken: stories, degree: 321\n",
      "Book: and then there were none, degree: 244\n",
      "Book: lipstick jungle, degree: 230\n",
      "\n",
      "5. largest community 3: \n",
      "Book: what we keep, degree: 370\n",
      "Book: suzanne's diary for nicholas, degree: 314\n",
      "Book: family pictures, degree: 306\n",
      "\n",
      "6. largest community 1: \n",
      "Book: the survivors club, degree: 271\n",
      "Book: i, alex cross, degree: 250\n",
      "Book: absolute fear, degree: 248\n",
      "\n",
      "7. largest community 0: \n",
      "Book: zoe's tale, degree: 242\n",
      "Book: one second after, degree: 202\n",
      "Book: shall we tell the president?, degree: 158\n",
      "\n",
      "8. largest community 7: \n",
      "Book: night world, no. 1, degree: 327\n",
      "Book: new moon, degree: 262\n",
      "Book: danse macabre, degree: 213\n",
      "\n",
      "9. largest community 2: \n",
      "Book: just one look, degree: 247\n",
      "Book: the sister, degree: 207\n",
      "Book: in too deep, degree: 200\n",
      "\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "shelf_int_graph = make_all_nodes_ints(shelf_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "shelf_top_3_books = get_top_3_books(shelf_int_graph, shelf_louvain)\n",
    "#print(shelf_top_3_books)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: [(968, 'harry potter and the chamber of secrets'),\n  (941, 'harry potter and the prisoner of azkaban'),\n  (613, 'harry potter and the deathly hallows')]}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shelf_top_3_books"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
